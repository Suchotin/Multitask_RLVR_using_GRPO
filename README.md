# Multitask RLVR using GRPO
### *HSE, "Data Science Specialist" Final Project*
**Sukhotin Oleg, DS-19, 2026**


В рамках данной работы исследуется применение методов обучения с подкреплением на основе верифицируемых вознаграждений (Reinforcement Learning with Verifiable Rewards, RLVR) для дообучения больших языковых моделей в задачах, допускающих автоматическую проверку корректности ответа. Основное внимание уделяется алгоритму Group Relative Policy Optimization (GRPO), который позволяет эффективно обучать модель по групповым генерациям без необходимости использования обученной модели-критика. Целью проекта является систематическое изучение того, как GRPO влияет на качество решений в различных формально проверяемых доменах, а также анализ переноса и интерференции между задачами при совместном обучении и объединении адаптеров.

В работе рассматриваются три класса задач с различной структурой и требованиями к верификации: решение математических задач, генерация SQL-запросов с проверкой выполнения, генерация Python-кода с автоматическим запуском и проверкой на тест-кейсах. Методологически проект строится вокруг единой инфраструктуры RLVR, в которой все задачи приводятся к общему формату взаимодействия модели с верификаторами. Генерации модели проверяются в sandbox-окружении, где для кода выполняется компиляция и запуск, для SQL — исполнение запросов на реальных базах данных, а для математики — синтаксический разбор и формальная проверка выражений. Итоговый reward вычисляется как взвешенная сумма компонент и используется в GRPO для обновления политики. Особое внимание уделяется инженерным аспектам: выравниванию размерностей при групповом семплировании, маскированию reward’ов в смешанных батчах и стабильному логированию компонент вознаграждения.

Экспериментальная часть включает сравнение нескольких стратегий обучения: моделей, дообученных GRPO на отдельных задачах, модели совместного мультизадачного обучения, а также моделей, полученных путём объединения (merge) адаптеров, обученных на разных задачах независимо. Оценка качества проводится на независимых бенчмарках, включая GSM8K, HumanEval, ARC-Challenge, HellaSwag и Spider, что позволяет анализировать как целевые улучшения, так и перенос между доменами. Дополнительно исследуется динамика обучения, стабильность reward’ов и характер ошибок, допускаемых моделью до и после применения RLVR.

# Обучающие кривые
### Математика
<img width="606" height="380" alt="image" src="https://github.com/user-attachments/assets/1757dd6d-5764-4ebc-b80f-566bf6772568" />

### Text2SQL
<img width="610" height="378" alt="image" src="https://github.com/user-attachments/assets/9cdea855-aff3-4ab2-86c8-547eb441cec2" />

### Verifiable Python
<img width="610" height="376" alt="image" src="https://github.com/user-attachments/assets/c99d1d00-3a99-445a-859a-1e75f2c471ea" />

### Multitask
<img width="696" height="457" alt="image" src="https://github.com/user-attachments/assets/56f7855f-df1d-4094-b1a8-9b75e4aa7570" />
