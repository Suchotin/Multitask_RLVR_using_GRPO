# Multitask RLVR using GRPO
### *HSE, "Data Science Specialist" Final Project*
**Sukhotin Oleg, DS-19, 2026**


В рамках данной работы исследуется применение методов обучения с подкреплением на основе верифицируемых вознаграждений (Reinforcement Learning with Verifiable Rewards, RLVR) для дообучения больших языковых моделей в задачах, допускающих автоматическую проверку корректности ответа. Основное внимание уделяется алгоритму Group Relative Policy Optimization (GRPO), который позволяет эффективно обучать модель по групповым генерациям без необходимости использования обученной модели-критика. Целью проекта является систематическое изучение того, как GRPO влияет на качество решений в различных формально проверяемых доменах, а также анализ переноса и интерференции между задачами при совместном обучении и объединении адаптеров.

В работе рассматриваются три класса задач с различной структурой и требованиями к верификации: решение математических задач, генерация SQL-запросов с проверкой выполнения, генерация Python-кода с автоматическим запуском и проверкой на тест-кейсах. Методологически проект строится вокруг единой инфраструктуры RLVR, в которой все задачи приводятся к общему формату взаимодействия модели с верификаторами. Генерации модели проверяются в sandbox-окружении, где для кода выполняется компиляция и запуск, для SQL — исполнение запросов на реальных базах данных, а для математики — синтаксический разбор и формальная проверка выражений. Итоговый reward вычисляется как взвешенная сумма компонент и используется в GRPO для обновления политики. Особое внимание уделяется инженерным аспектам: выравниванию размерностей при групповом семплировании, маскированию reward’ов в смешанных батчах и стабильному логированию компонент вознаграждения.

Экспериментальная часть включает сравнение нескольких стратегий обучения: моделей, дообученных GRPO на отдельных задачах, модели совместного мультизадачного обучения, а также моделей, полученных путём объединения (merge) адаптеров, обученных на разных задачах независимо. Оценка качества проводится на независимых бенчмарках, включая GSM8K, HumanEval, ARC-Challenge, HellaSwag и Spider, что позволяет анализировать как целевые улучшения, так и перенос между доменами. Дополнительно исследуется динамика обучения, стабильность reward’ов и характер ошибок, допускаемых моделью до и после применения RLVR.

# Обучающие кривые
- **License:** apache-2.0
- **Finetuned from model :** unsloth/qwen2.5-coder-7b-instruct-bnb-4bit

This qwen2 model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth)

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)

---

<img width="1611" height="770" alt="Screenshot 2026-02-09 at 04 26 30" src="https://github.com/user-attachments/assets/c4ab1cb1-d6da-4db8-b2cf-af2d914db402" />

---

<img width="1611" height="770" alt="Screenshot 2026-02-09 at 04 26 48" src="https://github.com/user-attachments/assets/e1a1af07-c0b0-4c6f-99ee-90bb5c32121f" />

---

<img width="1611" height="770" alt="Screenshot 2026-02-09 at 04 27 04" src="https://github.com/user-attachments/assets/5a1277df-a5c2-4f4c-a053-90a816c0a5e6" />

---

<img width="1611" height="770" alt="Screenshot 2026-02-09 at 04 27 15" src="https://github.com/user-attachments/assets/95a80a9e-933b-44f2-9ffa-6fe260a4085d" />

---
# Бенчмарки

<img width="1583" height="470" alt="Screenshot 2026-02-09 at 04 33 05" src="https://github.com/user-attachments/assets/4f8bea07-5573-453d-ae54-8effe4a29aff" />
