{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd815d39",
      "metadata": {
        "id": "dd815d39"
      },
      "source": [
        "# Multitask RLVR using GRPO\n",
        "### *HSE, \"Data Science Specialist\" Final Project*\n",
        "Sukhotin Oleg, DS-19, 2026\n",
        "\n",
        "**Цель проекта:** применить GRPO (Grouped Relative Policy Optimization) для дообучения модели **Qwen2.5-7B-Instruct** на задачах с *проверяемыми* ответами: математика, SQL, программирование (Python).\n",
        "\n",
        "**Ключевая идея:** используем **rull-based reward-функции**, которые верифицируют ответ (парсинг, запуск тестов, выполнение SQL-запроса и т.п.).  \n",
        "В результате получаем воспроизводимый пайплайн: *данные → предобработка → EDA → reward → обучение → валидация → результаты*.\n",
        "\n",
        "Ноутбук интегрирован с сервисами Google Drive, tensorboard (website via ngrok), huggingface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04b784cc",
      "metadata": {
        "id": "04b784cc"
      },
      "source": [
        "## 1. Окружение и зависимости\n",
        "\n",
        "Ниже — минимальный сетап для Colab (или любой Linux-среды).  \n",
        "Версии зафиксированы так же, как в экспериментальном окружении, чтобы результаты были воспроизводимы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "308f51e9",
      "metadata": {
        "id": "308f51e9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "\n",
        "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "!pip -q install --upgrade uv\n",
        "!uv pip install -qqq transformers==4.56.2\n",
        "!uv pip install -qqq --no-deps trl==0.22.2\n",
        "\n",
        "# Unsloth + vLLM (версии можно адаптировать под GPU/среду)\n",
        "!uv pip install -qqq unsloth vllm torchvision bitsandbytes xformers\n",
        "\n",
        "# Rewards / утилиты\n",
        "!uv pip install -qqq sqlglot math_verify latex2sympy2_extended\n",
        "\n",
        "# EDA / отчёты\n",
        "!uv pip install -qqq pandas tqdm matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e004dec3",
      "metadata": {
        "id": "e004dec3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount(\"/content/drive\")\n",
        "except Exception:\n",
        "    print(\"Google Colab не обнаружен — продолжаем без Drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pyngrok\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"PLACE_YOUR_TOKEN_HERE\")"
      ],
      "metadata": {
        "id": "9Nv2-Q7KCj_i"
      },
      "id": "9Nv2-Q7KCj_i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "acb98f94",
      "metadata": {
        "id": "acb98f94"
      },
      "source": [
        "## 2. Конфигурация проекта и воспроизводимость\n",
        "\n",
        "Здесь задаются пути, seed и базовые настройки.  \n",
        "Все артефакты (логи, eval-результаты, чекпоинты) сохраняются в `RUNS_DIR`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5a0e82",
      "metadata": {
        "id": "3f5a0e82"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "PROJECT_DIR = os.environ.get(\"PROJECT_DIR\", \"/content/drive/MyDrive/HSE GRPO\")\n",
        "DATASETS_DIR = f\"{PROJECT_DIR}/datasets\"\n",
        "SPLITS_DIR = f\"{PROJECT_DIR}/splits\"\n",
        "RUNS_DIR = f\"{PROJECT_DIR}/runs\"\n",
        "\n",
        "Path(DATASETS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(SPLITS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(RUNS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "RUN_STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
        "print(\"RUNS_DIR:\", RUNS_DIR)\n",
        "print(\"SEED:\", SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b6daf1",
      "metadata": {
        "id": "09b6daf1"
      },
      "source": [
        "## 3. Модель и LoRA-конфигурация\n",
        "\n",
        "Базовая модель: **unsloth/Qwen2.5-7B-Instruct-bnb-4bit**.  \n",
        "Для обучения используем LoRA (PEFT) — это существенно экономит память и ускоряет итерации.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsloth TRL Patch\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:898/0*tporAgJg5LLllQe_.png\" width=250>\n",
        "\n",
        "Unsloth's `PatchFastRL` modifies TRL (Transformer Reinforcement Learning) trainers to work efficiently with Unsloth's optimized models. The patch makes three key changes:\n",
        "\n",
        "1. Patches the model unwrapping process for efficient generation during training\n",
        "2. Modifies TRL trainers with Unsloth-specific optimizations, including things like:\n",
        "   - Automatically handles mixed precision training (FP16/BF16) based on model config\n",
        "   - Sets optimal defaults for hyperparameters like learning rate, batch sizes, and gradient accumulation\n",
        "   - Adds safety checks for learning rates and sequence lengths\n",
        "   - Configures tokenizer padding and model inference settings\n",
        "   - Integrates with vLLM for faster inference when available\n",
        "   - And more!\n",
        "3. Sets up algorithm-specific statistics tracking (in our case, for GRPO)\n",
        "\n",
        "You can check out the implementation details [in their code here](https://github.com/unslothai/unsloth/blob/d1d15f1d14f1168837d29b9c08e9b6d63945d469/unsloth/models/rl.py)."
      ],
      "metadata": {
        "id": "3im7nQ6QFFlC"
      },
      "id": "3im7nQ6QFFlC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863400c9",
      "metadata": {
        "id": "863400c9"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0f55ee",
      "metadata": {
        "id": "ea0f55ee"
      },
      "outputs": [],
      "source": [
        "# max_seq_length: должен покрывать prompt + completion.\n",
        "# load_in_4bit: экономия VRAM.\n",
        "# gpu_memory_utilization: ограничиваем, чтобы избежать OOM в Colab.\n",
        "\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=4096,\n",
        "    load_in_4bit=True,\n",
        "    fast_inference=True,\n",
        "    max_lora_rank=64,\n",
        "    gpu_memory_utilization=0.30,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "print(\"Loaded:\", MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ce0801",
      "metadata": {
        "id": "47ce0801"
      },
      "source": [
        "## 4. Данные: источники, формат, промпты\n",
        "\n",
        "Все задачи приводятся к единому формату для TRL/GRPO:\n",
        "\n",
        "- `prompt`: список сообщений (system/user) в стиле chat-template\n",
        "- `task`: строка (`sql` | `math` | `sudoku` | `code` | `ioi`)\n",
        "- `solution`: золотой ответ (строка)\n",
        "- `references`: JSON-строка с доп.информацией для reward (тесты, контекст, метаданные)\n",
        "\n",
        "**Важно:** `references` всегда строка. Это критично для безопасной конкатенации датасетов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19b4608a",
      "metadata": {
        "id": "19b4608a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "SYSTEM_PROMPT = 'You are a helpful assistant.\\\\n\\\\nYou MUST answer in exactly this structure (no text outside these tags):\\\\n\\\\n<reasoning>\\\\n...your step-by-step reasoning...\\\\n</reasoning>\\\\n<answer>\\\\n...final output only (see task-specific rules)...\\\\n</answer>\\\\n\\\\nHARD REQUIREMENTS (apply to ALL tasks):\\\\n- <reasoning> is REQUIRED AND MUST be CLOSED with </reasoning>.\\\\n- <answer> is REQUIRED AND MUST be CLOSED with </answer>.\\\\n- Do not output anything before <reasoning> or after </answer>.\\\\n- Do not use extra wrapper tags besides <reasoning> and <answer>.\\\\n\\\\nTask-specific rules (content inside <answer>):\\\\n- SQL: output ONLY ONE SQL query. No explanation, no markdown/code fences.\\\\n- MATH: output ONLY the final result (no extra text).\\\\n- SUDOKU (4x4): output ONLY the solved 4x4 grid (spaces between numbers, newline between rows).\\\\n- CODE (Python): output ONLY ONE fenced code block:\\\\n    ```python\\\\n    ...\\\\n    ```\\\\n  Nothing else inside <answer>.\\\\n- IOI (C++17): output ONLY ONE fenced code block:\\\\n    ```cpp\\\\n    ...\\\\n    ```\\\\n  Nothing else inside <answer>.'\n",
        "\n",
        "\n",
        "def ref_json(obj: Any) -> str:\n",
        "    \"\"\"Serialize references into a JSON string (stable schema for concatenation).\"\"\"\n",
        "    try:\n",
        "        return json.dumps(obj, ensure_ascii=False, default=str)\n",
        "    except Exception:\n",
        "        return json.dumps(str(obj), ensure_ascii=False)\n",
        "\n",
        "\n",
        "def read_jsonl(path: str, limit: int = 0) -> List[Dict[str, Any]]:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "            if limit and len(rows) >= limit:\n",
        "                break\n",
        "    return rows\n",
        "\n",
        "\n",
        "def sql_key(ex: Dict[str, Any]) -> tuple[str, str, str]:\n",
        "    return (ex[\"sql_prompt\"].strip(), ex[\"sql_context\"].strip(), ex[\"sql\"].strip())\n",
        "\n",
        "\n",
        "def validate_grpo_dataset(ds: Dataset, expected_task: Optional[str] = None, n_check: int = 50) -> None:\n",
        "    \"\"\"Hard validation of GRPO dataset schema. Raises on mismatch.\"\"\"\n",
        "    required = {\"prompt\", \"task\", \"solution\", \"references\"}\n",
        "    cols = set(ds.column_names)\n",
        "    missing = required - cols\n",
        "    if missing:\n",
        "        raise ValueError(f\"Dataset missing columns {missing}. Has: {ds.column_names}\")\n",
        "\n",
        "    n = min(len(ds), n_check)\n",
        "    for i in range(n):\n",
        "        ex = ds[i]\n",
        "        if expected_task is not None and ex[\"task\"] != expected_task:\n",
        "            raise ValueError(f\"Row {i}: task={ex['task']} but expected {expected_task}\")\n",
        "\n",
        "        if not isinstance(ex[\"solution\"], str):\n",
        "            raise TypeError(f\"Row {i}: solution must be str, got {type(ex['solution'])}\")\n",
        "\n",
        "        if not isinstance(ex[\"references\"], str):\n",
        "            raise TypeError(f\"Row {i}: references must be str, got {type(ex['references'])}\")\n",
        "\n",
        "        p = ex[\"prompt\"]\n",
        "        if not isinstance(p, list) or not p:\n",
        "            raise TypeError(f\"Row {i}: prompt must be non-empty list, got {type(p)}\")\n",
        "        for j, msg in enumerate(p):\n",
        "            if not isinstance(msg, dict):\n",
        "                raise TypeError(f\"Row {i} msg {j}: must be dict, got {type(msg)}\")\n",
        "            if \"role\" not in msg or \"content\" not in msg:\n",
        "                raise ValueError(f\"Row {i} msg {j}: missing role/content keys\")\n",
        "            if not isinstance(msg[\"role\"], str) or not isinstance(msg[\"content\"], str):\n",
        "                raise TypeError(f\"Row {i} msg {j}: role/content must be str\")\n",
        "\n",
        "    print(\n",
        "        f\"✅ validate_grpo_dataset OK: rows={len(ds)} cols={ds.column_names} task={expected_task or 'mixed'}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbebe15d",
      "metadata": {
        "id": "bbebe15d"
      },
      "source": [
        "### 4.1 Построение примеров для каждой задачи\n",
        "\n",
        "Функции `build_*_samples` приводят разные датасеты к единому формату.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82f33485",
      "metadata": {
        "id": "82f33485"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def build_sql_samples(sql_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    samples: List[Dict[str, Any]] = []\n",
        "    for ex in sql_rows:\n",
        "        sql_prompt = ex[\"sql_prompt\"]\n",
        "        sql_context = ex[\"sql_context\"]\n",
        "        gold_sql = ex[\"sql\"]\n",
        "\n",
        "        samples.append(\n",
        "            {\n",
        "                \"prompt\": [\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"TASK: SQL\\n\" + f\"{sql_context}\\n\\nQuestion: {sql_prompt}\",\n",
        "                    },\n",
        "                ],\n",
        "                \"task\": \"sql\",\n",
        "                \"solution\": gold_sql,\n",
        "                \"references\": ref_json(\n",
        "                    {\n",
        "                        \"sql_prompt\": sql_prompt,\n",
        "                        \"sql_context\": sql_context,\n",
        "                        \"gold_sql\": gold_sql,\n",
        "                    }\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "    return samples\n",
        "\n",
        "\n",
        "def build_math_samples(math_ds, limit: int = 0) -> List[Dict[str, Any]]:\n",
        "    samples: List[Dict[str, Any]] = []\n",
        "    for i, ex in enumerate(math_ds):\n",
        "        problem = ex[\"prompt\"]\n",
        "        gold_answer = ex[\"solution\"]\n",
        "        samples.append(\n",
        "            {\n",
        "                \"prompt\": [\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": \"TASK: MATH\\n\\n\" + problem},\n",
        "                ],\n",
        "                \"task\": \"math\",\n",
        "                \"solution\": gold_answer,\n",
        "                \"references\": \"\",\n",
        "            }\n",
        "        )\n",
        "        if limit and (i + 1) >= limit:\n",
        "            break\n",
        "    return samples\n",
        "\n",
        "\n",
        "def build_code_samples(code_ds, limit: int = 0) -> List[Dict[str, Any]]:\n",
        "    samples: List[Dict[str, Any]] = []\n",
        "    for i, ex in enumerate(code_ds):\n",
        "        statement = ex.get(\"problem_statement\") or ex.get(\"prompt\") or ex.get(\"question\") or \"\"\n",
        "        verification_info = ex.get(\"verification_info\") or {}\n",
        "        pid = ex.get(\"id\") or ex.get(\"problem_id\") or ex.get(\"slug\") or ex.get(\"name\") or \"\"\n",
        "\n",
        "        gold = ex.get(\"solution\") or ex.get(\"gold_solution\") or ex.get(\"reference_solution\") or ex.get(\"answer\") or \"\"\n",
        "\n",
        "        samples.append(\n",
        "            {\n",
        "                \"prompt\": [\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": \"TASK: CODE\\n\\n\" + str(statement).strip()},\n",
        "                ],\n",
        "                \"task\": \"code\",\n",
        "                \"solution\": str(gold) if gold is not None else \"\",\n",
        "                \"references\": ref_json({\"id\": pid, \"verification_info\": verification_info}),\n",
        "            }\n",
        "        )\n",
        "        if limit and (i + 1) >= limit:\n",
        "            break\n",
        "    return samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8338458",
      "metadata": {
        "id": "b8338458"
      },
      "source": [
        "### 4.2 Детерминированные сплиты для OpenR1-Math (one-time)\n",
        "\n",
        "Чтобы сравнения baseline/post-train были корректными, фиксируем eval-набор через сохранённые индексы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce88a6dd",
      "metadata": {
        "id": "ce88a6dd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "MATH_CONFIG = \"level_3\"\n",
        "MATH_DATASET = \"open-r1/Big-Math-RL-Verified-Processed\"\n",
        "MATH_EVAL_N = 1000\n",
        "\n",
        "math_train_split_file = Path(SPLITS_DIR) / f\"openr1_math_{MATH_CONFIG}_train_seed{SEED}.json\"\n",
        "math_eval_split_file = Path(SPLITS_DIR) / f\"openr1_math_{MATH_CONFIG}_eval_{MATH_EVAL_N}_seed{SEED}.json\"\n",
        "\n",
        "if not (math_train_split_file.exists() and math_eval_split_file.exists()):\n",
        "    ds = load_dataset(MATH_DATASET, MATH_CONFIG, split=\"train\")\n",
        "    n = len(ds)\n",
        "\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    perm = rng.permutation(n).tolist()\n",
        "\n",
        "    eval_n = min(MATH_EVAL_N, n)\n",
        "    eval_idx = perm[:eval_n]\n",
        "    train_idx = perm[eval_n:]\n",
        "\n",
        "    math_train_split_file.write_text(json.dumps({\"train_indices\": train_idx}, ensure_ascii=False), encoding=\"utf-8\")\n",
        "    math_eval_split_file.write_text(json.dumps({\"eval_indices\": eval_idx}, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Saved split files:\")\n",
        "    print(\"-\", math_train_split_file)\n",
        "    print(\"-\", math_eval_split_file)\n",
        "else:\n",
        "    print(\"Split files already exist:\")\n",
        "    print(\"-\", math_train_split_file)\n",
        "    print(\"-\", math_eval_split_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d075bba",
      "metadata": {
        "id": "7d075bba"
      },
      "source": [
        "### 4.3 Сборка train/eval датасетов\n",
        "\n",
        "- SQL берём из локальных JSONL (предполагается, что вы уже подготовили LLM-generated dataset).\n",
        "- Остальные задачи — напрямую из HuggingFace Datasets.\n",
        "- Для SQL дополнительно фильтруем пересечение train/eval (избегаем утечек).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d96df24",
      "metadata": {
        "id": "0d96df24"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from datasets import concatenate_datasets, interleave_datasets\n",
        "\n",
        "ENABLE_SQL = True\n",
        "ENABLE_MATH = True\n",
        "ENABLE_CODE = True\n",
        "\n",
        "SQL_TRAIN_JSONL_PATH = f\"{DATASETS_DIR}/cleaned_train_queries.jsonl\"\n",
        "SQL_EVAL_JSONL_PATH = f\"{DATASETS_DIR}/cleaned_eval_queries.jsonl\"\n",
        "\n",
        "SQL_TRAIN_LIMIT = 900\n",
        "MATH_TRAIN_LIMIT = 900\n",
        "CODE_TRAIN_LIMIT = 900\n",
        "\n",
        "SQL_EVAL_FULL_N = 1000\n",
        "SQL_EVAL_BASELINE_N = 200\n",
        "MATH_EVAL_BASELINE_N = 200\n",
        "CODE_EVAL_FULL_N = 500\n",
        "CODE_EVAL_BASELINE_N = 200\n",
        "\n",
        "\n",
        "def empty_grpo_ds() -> Dataset:\n",
        "    return Dataset.from_dict({\"prompt\": [], \"task\": [], \"solution\": [], \"references\": []})\n",
        "\n",
        "\n",
        "train_parts = []\n",
        "sql_train_grpo_ds = empty_grpo_ds()\n",
        "math_train_grpo_ds = empty_grpo_ds()\n",
        "code_train_grpo_ds = empty_grpo_ds()\n",
        "\n",
        "if ENABLE_SQL:\n",
        "    if not (Path(SQL_TRAIN_JSONL_PATH).exists() and Path(SQL_EVAL_JSONL_PATH).exists()):\n",
        "        raise FileNotFoundError(\n",
        "            \"Не найдены SQL jsonl файлы. Ожидается:\\n\"\n",
        "            f\"- {SQL_TRAIN_JSONL_PATH}\\n- {SQL_EVAL_JSONL_PATH}\"\n",
        "        )\n",
        "\n",
        "    sql_train_rows = read_jsonl(SQL_TRAIN_JSONL_PATH, limit=SQL_TRAIN_LIMIT)\n",
        "    sql_eval_rows_for_filter = read_jsonl(SQL_EVAL_JSONL_PATH, limit=0)\n",
        "\n",
        "    eval_keys = set(sql_key(x) for x in sql_eval_rows_for_filter)\n",
        "    before = len(sql_train_rows)\n",
        "    sql_train_rows = [x for x in sql_train_rows if sql_key(x) not in eval_keys]\n",
        "    after = len(sql_train_rows)\n",
        "\n",
        "    print(f\"SQL train overlap filtered: {before} -> {after}\")\n",
        "    sql_train_grpo_ds = Dataset.from_list(build_sql_samples(sql_train_rows))\n",
        "    validate_grpo_dataset(sql_train_grpo_ds, expected_task=\"sql\")\n",
        "    train_parts.append(sql_train_grpo_ds)\n",
        "\n",
        "if ENABLE_MATH:\n",
        "    train_idx = json.loads(Path(math_train_split_file).read_text(encoding=\"utf-8\"))[\"train_indices\"]\n",
        "    math_raw_ds = load_dataset(MATH_DATASET, MATH_CONFIG, split=\"train\").select(train_idx)\n",
        "    if MATH_TRAIN_LIMIT and MATH_TRAIN_LIMIT < len(math_raw_ds):\n",
        "        math_raw_ds = math_raw_ds.select(range(MATH_TRAIN_LIMIT))\n",
        "    math_train_grpo_ds = Dataset.from_list(build_math_samples(math_raw_ds))\n",
        "    validate_grpo_dataset(math_train_grpo_ds, expected_task=\"math\")\n",
        "    train_parts.append(math_train_grpo_ds)\n",
        "\n",
        "if ENABLE_CODE:\n",
        "    code_raw_ds = load_dataset(\"open-r1/verifiable-coding-problems-python\", split=\"train\")\n",
        "    if CODE_TRAIN_LIMIT and CODE_TRAIN_LIMIT < len(code_raw_ds):\n",
        "        code_raw_ds = code_raw_ds.select(range(CODE_TRAIN_LIMIT))\n",
        "    code_train_grpo_ds = Dataset.from_list(build_code_samples(code_raw_ds))\n",
        "    validate_grpo_dataset(code_train_grpo_ds, expected_task=\"code\")\n",
        "    train_parts.append(code_train_grpo_ds)\n",
        "\n",
        "if not train_parts:\n",
        "    raise RuntimeError(\"Не включена ни одна задача — train_parts пустой.\")\n",
        "\n",
        "parts_shuffled = [d.shuffle(seed=SEED) for d in train_parts]\n",
        "probs = [1.0 / len(parts_shuffled)] * len(parts_shuffled)\n",
        "train_dataset = interleave_datasets(\n",
        "    parts_shuffled,\n",
        "    probabilities=probs,\n",
        "    seed=SEED,\n",
        "    stopping_strategy=\"all_exhausted\",\n",
        ")\n",
        "\n",
        "print(\"\\n✅ TRAIN ready\")\n",
        "for d in parts_shuffled:\n",
        "    print(\"-\", d[0][\"task\"], \"size:\", len(d))\n",
        "print(\"mixed train size:\", len(train_dataset))\n",
        "\n",
        "\n",
        "# --- EVAL datasets ---\n",
        "sql_eval_samples_full = []\n",
        "sql_eval_samples_baseline = []\n",
        "math_eval_samples_full = []\n",
        "math_eval_samples_baseline = []\n",
        "code_eval_samples_full = []\n",
        "code_eval_samples_baseline = []\n",
        "\n",
        "if ENABLE_SQL:\n",
        "    sql_eval_rows_full = read_jsonl(SQL_EVAL_JSONL_PATH, limit=SQL_EVAL_FULL_N)\n",
        "    sql_eval_rows_baseline = sql_eval_rows_full[:SQL_EVAL_BASELINE_N]\n",
        "    sql_eval_samples_full = build_sql_samples(sql_eval_rows_full)\n",
        "    sql_eval_samples_baseline = build_sql_samples(sql_eval_rows_baseline)\n",
        "\n",
        "if ENABLE_MATH:\n",
        "    eval_idx = json.loads(Path(math_eval_split_file).read_text(encoding=\"utf-8\"))[\"eval_indices\"]\n",
        "    math_eval_full_raw = load_dataset(MATH_DATASET, MATH_CONFIG, split=\"train\").select(eval_idx)\n",
        "    math_eval_baseline_raw = math_eval_full_raw.select(range(min(MATH_EVAL_BASELINE_N, len(math_eval_full_raw))))\n",
        "    math_eval_samples_full = build_math_samples(math_eval_full_raw)\n",
        "    math_eval_samples_baseline = build_math_samples(math_eval_baseline_raw)\n",
        "\n",
        "if ENABLE_CODE:\n",
        "    code_eval_raw = load_dataset(\"open-r1/verifiable-coding-problems-python\", split=\"train\")\n",
        "    code_eval_full_raw = code_eval_raw.select(range(min(CODE_EVAL_FULL_N, len(code_eval_raw))))\n",
        "    code_eval_baseline_raw = code_eval_full_raw.select(range(min(CODE_EVAL_BASELINE_N, len(code_eval_full_raw))))\n",
        "    code_eval_samples_full = build_code_samples(code_eval_full_raw)\n",
        "    code_eval_samples_baseline = build_code_samples(code_eval_baseline_raw)\n",
        "\n",
        "eval_dataset_full = Dataset.from_list(\n",
        "    sql_eval_samples_full\n",
        "    + math_eval_samples_full\n",
        "    + code_eval_samples_full\n",
        ")\n",
        "\n",
        "eval_dataset_baseline = Dataset.from_list(\n",
        "    sql_eval_samples_baseline\n",
        "    + math_eval_samples_baseline\n",
        "    + code_eval_samples_baseline\n",
        ")\n",
        "\n",
        "print(\"\\n✅ EVAL ready\")\n",
        "print(\"Eval baseline total:\", len(eval_dataset_baseline))\n",
        "print(\"Eval full total:\", len(eval_dataset_full))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1d554b",
      "metadata": {
        "id": "6b1d554b"
      },
      "source": [
        "## 5. EDA: быстрые sanity-проверки датасетов\n",
        "\n",
        "Ключевые аспекты EDA для RL-задач:\n",
        "\n",
        "- размеры поднаборов по задачам;\n",
        "- длина prompt’ов (в токенах) → влияет на `max_prompt_length` и VRAM;\n",
        "- несколько примеров для ручной проверки формата.\n",
        "\n",
        "Ниже — сводка по длинам prompt’ов на небольшом сэмпле.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d9cf96e",
      "metadata": {
        "id": "8d9cf96e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def prompt_to_text(prompt_messages: List[Dict[str, str]]) -> str:\n",
        "    return tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "\n",
        "def summarize_ds(ds: Dataset, name: str, sample_n: int = 200) -> Dict[str, Any]:\n",
        "    n = len(ds)\n",
        "    m = min(sample_n, n)\n",
        "    texts = [prompt_to_text(ds[i][\"prompt\"]) for i in range(m)]\n",
        "    tok_lens = [len(tokenizer(t).input_ids) for t in texts]\n",
        "    char_lens = [len(t) for t in texts]\n",
        "    return {\n",
        "        \"task\": name,\n",
        "        \"n_rows\": n,\n",
        "        \"sample_n\": m,\n",
        "        \"prompt_tokens_mean\": float(np.mean(tok_lens)) if tok_lens else 0.0,\n",
        "        \"prompt_tokens_p95\": float(np.quantile(tok_lens, 0.95)) if tok_lens else 0.0,\n",
        "        \"prompt_chars_mean\": float(np.mean(char_lens)) if char_lens else 0.0,\n",
        "    }\n",
        "\n",
        "\n",
        "rows = []\n",
        "for ds_name, ds_obj in [\n",
        "    (\"sql_train\", sql_train_grpo_ds),\n",
        "    (\"math_train\", math_train_grpo_ds),\n",
        "    (\"code_train\", code_train_grpo_ds),\n",
        "]:\n",
        "    if len(ds_obj) > 0:\n",
        "        rows.append(summarize_ds(ds_obj, ds_name))\n",
        "\n",
        "summary_df = pd.DataFrame(rows).sort_values(\"n_rows\", ascending=False)\n",
        "summary_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d952fd95",
      "metadata": {
        "id": "d952fd95"
      },
      "outputs": [],
      "source": [
        "if not summary_df.empty:\n",
        "    ax = summary_df.plot(kind=\"bar\", x=\"task\", y=\"prompt_tokens_mean\", legend=False)\n",
        "    ax.set_title(\"Средняя длина prompt (токены) по задачам\")\n",
        "    ax.set_ylabel(\"tokens\")\n",
        "    plt.xticks(rotation=30, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e3a57b3",
      "metadata": {
        "id": "4e3a57b3"
      },
      "source": [
        "Примеры (по одному) — полезны для ручной проверки структуры prompt и полей `references/solution`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da2768b8",
      "metadata": {
        "id": "da2768b8"
      },
      "outputs": [],
      "source": [
        "def show_example(ds: Dataset, i: int = 0) -> None:\n",
        "    ex = ds[i]\n",
        "    print(\"task:\", ex[\"task\"])\n",
        "    sol = ex[\"solution\"]\n",
        "    ref = ex[\"references\"]\n",
        "    print(\"solution head:\", (sol[:120] + \"...\") if len(sol) > 120 else sol)\n",
        "    print(\"references head:\", (ref[:160] + \"...\") if len(ref) > 160 else ref)\n",
        "    print(\"\\n--- prompt (last user) ---\")\n",
        "    print(ex[\"prompt\"][-1][\"content\"][:800])\n",
        "\n",
        "\n",
        "for name, ds in [\n",
        "    (\"sql\", sql_train_grpo_ds),\n",
        "    (\"math\", math_train_grpo_ds),\n",
        "    (\"code\", code_train_grpo_ds),\n",
        "]:\n",
        "    if len(ds) > 0:\n",
        "        print(\"\\n====================\", name, \"====================\")\n",
        "        show_example(ds, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db1a13e",
      "metadata": {
        "id": "1db1a13e"
      },
      "source": [
        "## 6. Reward-функции (verifiable)\n",
        "\n",
        "Компоненты reward:\n",
        "\n",
        "- **Format reward** (строгая разметка `<reasoning>...</reasoning><answer>...</answer>`, а для code — fenced code block).\n",
        "- **Task-specific reward**:\n",
        "  - **Math**: `math_verify` (парсинг LaTeX/выражений и проверка эквивалентности).\n",
        "  - **SQL**: парсинг/нормализация + выполнение запросов (execution-based reward).\n",
        "  - **Code (Python)**: запуск тестов из `verification_info`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e49731f",
      "metadata": {
        "id": "2e49731f"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any\n",
        "\n",
        "def completion_to_text(completion: Any) -> str:\n",
        "    if completion is None:\n",
        "        return \"\"\n",
        "    if isinstance(completion, str):\n",
        "        return completion\n",
        "    if isinstance(completion, dict):\n",
        "        return completion.get(\"content\") or completion.get(\"generated_text\") or completion.get(\"text\") or \"\"\n",
        "    if isinstance(completion, list) and completion:\n",
        "        return completion_to_text(completion[0])\n",
        "    return str(completion)\n",
        "\n",
        "_get_completion_text = completion_to_text\n",
        "_comp_to_text = completion_to_text\n",
        "\n",
        "_OPEN_RE = re.compile(r\"<answer\\s*>\", re.IGNORECASE)\n",
        "_CLOSE_RE = re.compile(r\"</answer\\s*>\", re.IGNORECASE)\n",
        "\n",
        "def extract_answer(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    m_open = _OPEN_RE.search(text)\n",
        "    if not m_open:\n",
        "        return \"\"\n",
        "    rest = text[m_open.end():]\n",
        "    m_close = _CLOSE_RE.search(rest)\n",
        "    return (rest[: m_close.start()] if m_close else rest).strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdfc7bca",
      "metadata": {
        "id": "cdfc7bca"
      },
      "source": [
        "### 6.1 SQL rewards\n",
        "\n",
        "Reward основан на:\n",
        "- извлечении SQL из `<answer>...</answer>`;\n",
        "- нормализации (sqlglot);\n",
        "- попытке выполнения запроса на базе и оценке результата;\n",
        "- эвристике сложности/качества reasoning.\n",
        "\n",
        "Код ниже самодостаточный и используется как зависимость для mixed-обучения.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "000af727",
      "metadata": {
        "id": "000af727"
      },
      "outputs": [],
      "source": [
        "import sqlglot\n",
        "from sqlglot import exp\n",
        "import re\n",
        "import os\n",
        "import tempfile\n",
        "import sqlite3\n",
        "import logging\n",
        "import sqlparse\n",
        "import torch\n",
        "import copy\n",
        "import sqlglot\n",
        "from sqlglot import parse as sqlglot_parse\n",
        "from sqlglot import transpile as sqlglot_transpile\n",
        "from sqlglot.errors import ParseError as SQLGlotParseError\n",
        "from sqlglot.expressions import Column\n",
        "from sqlglot.expressions import Column, Table\n",
        "from typing import List, Dict, Tuple, Any, Optional, Set, Union\n",
        "from sqlglot import transpile\n",
        "from sqlglot.errors import ParseError\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "def _ref_to_dict(ref) -> dict:\n",
        "    \"\"\"Normalize a single references item to dict.\n",
        "\n",
        "    After dataset concatenation we store references as a JSON string for ALL tasks.\n",
        "    This helper keeps backward-compatibility with legacy list/dict formats.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if ref is None:\n",
        "            return {}\n",
        "        if isinstance(ref, str):\n",
        "            s = ref.strip()\n",
        "            if not s:\n",
        "                return {}\n",
        "            try:\n",
        "                v = json.loads(s)\n",
        "                return v if isinstance(v, dict) else {}\n",
        "            except Exception:\n",
        "                return {}\n",
        "        if isinstance(ref, dict):\n",
        "            return ref\n",
        "        if isinstance(ref, list) and ref:\n",
        "            x = ref[0]\n",
        "            if isinstance(x, dict):\n",
        "                return x\n",
        "            if isinstance(x, str):\n",
        "                try:\n",
        "                    v = json.loads(x)\n",
        "                    return v if isinstance(v, dict) else {}\n",
        "                except Exception:\n",
        "                    return {}\n",
        "        return {}\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "\n",
        "def _sql_ref(references, i: int) -> dict:\n",
        "    try:\n",
        "        if references is None or i >= len(references):\n",
        "            return {}\n",
        "        return _ref_to_dict(references[i])\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "log_level = logging.DEBUG if os.environ.get(\n",
        "    \"SQL_DEBUG_MODE\") == \"1\" else logging.CRITICAL + 1\n",
        "logging.basicConfig(\n",
        "    level=log_level,\n",
        "    format='%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "REWARD_WEIGHTS = {\n",
        "    \"sql_correctness\": 1.0,\n",
        "    \"complexity\": 0.6,\n",
        "    \"reasoning\": 0.7,\n",
        "}\n",
        "\n",
        "DEBUG_MODE = os.environ.get(\"SQL_DEBUG_MODE\") == \"1\"\n",
        "\n",
        "ERR_SYNTAX = \"syntax_error\"\n",
        "ERR_MISSING_TABLE = \"missing_table\"\n",
        "ERR_MISSING_COLUMN = \"missing_column\"\n",
        "ERR_AMBIGUOUS_COLUMN = \"ambiguous_column\"\n",
        "ERR_TYPE_MISMATCH = \"type_mismatch\"\n",
        "ERR_CONSTRAINT = \"constraint_violation\"\n",
        "ERR_FUNCTION = \"function_error\"\n",
        "ERR_RESOURCE = \"resource_error\"\n",
        "ERR_OTHER = \"other_error\"\n",
        "ERR_SCHEMA_SETUP = \"schema_setup_error\"\n",
        "ERR_CONVERSION = \"sql_conversion_error\"\n",
        "ERR_EXECUTION = \"sql_execution_error\"\n",
        "ERR_SCHEMA_VALIDATION = \"schema_validation_error\"\n",
        "\n",
        "\n",
        "def _select_signature_ignore_aliases(sql: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Возвращает канонический список выражений в SELECT, игнорируя алиасы:\n",
        "    SELECT a AS x, b AS y  -> [\"a\", \"b\"]\n",
        "    Плюс старается нормализовать table-alias -> table-name (u.id == users.id).\n",
        "    \"\"\"\n",
        "    if not sql:\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        tree = sqlglot.parse_one(sql, read=\"sqlite\")\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "    # alias таблиц:  users u  ->  u -> users\n",
        "    table_alias_map = {}\n",
        "    try:\n",
        "        for t in tree.find_all(exp.Table):\n",
        "            alias = t.alias\n",
        "            if alias and alias != t.name:\n",
        "                table_alias_map[alias] = t.name\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    sel = tree.find(exp.Select) if not isinstance(tree, exp.Select) else tree\n",
        "    if not sel:\n",
        "        return []\n",
        "\n",
        "    signature = []\n",
        "    for proj in sel.expressions:\n",
        "        expr0 = proj.this if isinstance(proj, exp.Alias) else proj\n",
        "\n",
        "        # копия, чтобы безопасно править\n",
        "        try:\n",
        "            expr1 = expr0.copy()\n",
        "        except Exception:\n",
        "            expr1 = expr0\n",
        "\n",
        "        # нормализуем table alias в колонках: u.id -> users.id\n",
        "        try:\n",
        "            for col in expr1.find_all(exp.Column):\n",
        "                if col.table and col.table in table_alias_map:\n",
        "                    # table — это строка-идентификатор в sqlglot Column\n",
        "                    col.args[\"table\"] = exp.Identifier(this=table_alias_map[col.table])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # канонизируем выражение в строку\n",
        "        try:\n",
        "            signature.append(expr1.sql(dialect=\"sqlite\", normalize=True).lower())\n",
        "        except Exception:\n",
        "            signature.append(str(expr1).lower())\n",
        "\n",
        "    return signature\n",
        "\n",
        "\n",
        "def _match_positions_by_signature(gold_sig: list[str], gen_sig: list[str]) -> list[tuple[int, int]]:\n",
        "    used = set()\n",
        "    sig_to_gen = {}\n",
        "    for j, s in enumerate(gen_sig):\n",
        "        sig_to_gen.setdefault(s, []).append(j)\n",
        "\n",
        "    pairs = []\n",
        "    for i, s in enumerate(gold_sig):\n",
        "        for j in sig_to_gen.get(s, []):\n",
        "            if j not in used:\n",
        "                used.add(j)\n",
        "                pairs.append((i, j))\n",
        "                break\n",
        "    return pairs\n",
        "\n",
        "def _get_response_text(completion: Any) -> str:\n",
        "    response_text = \"\"\n",
        "    if isinstance(completion, str):\n",
        "        response_text = completion\n",
        "    elif isinstance(completion, list) and completion:\n",
        "        if isinstance(completion[0], dict):\n",
        "            response_text = completion[0].get(\n",
        "                'content', completion[0].get('generated_text', ''))\n",
        "        elif isinstance(completion[0], str):\n",
        "            response_text = completion[0]\n",
        "    elif isinstance(completion, dict):\n",
        "        response_text = completion.get(\n",
        "            'content', completion.get('generated_text', ''))\n",
        "    else:\n",
        "        try:\n",
        "            response_text = str(completion)\n",
        "        except Exception:\n",
        "            response_text = \"\"\n",
        "            logger.debug(\n",
        "                \"Could not convert completion to string: %s\", type(completion))\n",
        "    return response_text\n",
        "\n",
        "def extract_sql(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Логика для <answer>\n",
        "    if \"<answer>\" in text:\n",
        "        # Берем всё, что после открывающего тега\n",
        "        sql = text.split(\"<answer>\", 1)[1]\n",
        "\n",
        "        # Если есть закрывающий тег, обрезаем по нему\n",
        "        if \"</answer>\" in sql:\n",
        "            sql = sql.split(\"</answer>\", 1)[0]\n",
        "\n",
        "        # Очистка от комментариев (инлайн, без отдельной функции)\n",
        "        sql = re.sub(r\"^\\s*--.*?$\", \"\", sql, flags=re.MULTILINE)\n",
        "        sql = re.sub(r\"/\\*.*?\\*/\", \"\", sql, flags=re.DOTALL)\n",
        "        return sql.strip()\n",
        "\n",
        "    # Fallback метод (если тегов нет)\n",
        "    sql_keywords = [r\"\\bSELECT\\b\", r\"\\bINSERT\\b\", r\"\\bUPDATE\\b\",\n",
        "                    r\"\\bDELETE\\b\", r\"\\bCREATE\\b\", r\"\\bALTER\\b\", r\"\\bDROP\\b\"]\n",
        "\n",
        "    text_upper = text.upper()\n",
        "    sql_start_index = -1\n",
        "\n",
        "    for keyword in sql_keywords:\n",
        "        match = re.search(keyword, text_upper)\n",
        "        if match:\n",
        "            idx = match.start()\n",
        "            if sql_start_index == -1 or idx < sql_start_index:\n",
        "                sql_start_index = idx\n",
        "\n",
        "    if sql_start_index != -1:\n",
        "        potential_sql = text[sql_start_index:]\n",
        "\n",
        "        # Обрезаем лишнее, если попалось\n",
        "        potential_sql = potential_sql.split(\"</reasoning>\", 1)[0]\n",
        "\n",
        "        if \";\" in potential_sql:\n",
        "            potential_sql = potential_sql.split(\";\", 1)[0] + \";\"\n",
        "\n",
        "        # Очистка от комментариев\n",
        "        potential_sql = re.sub(r\"^\\s*--.*?$\", \"\", potential_sql, flags=re.MULTILINE)\n",
        "        potential_sql = re.sub(r\"/\\*.*?\\*/\", \"\", potential_sql, flags=re.DOTALL)\n",
        "\n",
        "        logger.debug(\"Extracted SQL using fallback method.\")\n",
        "        return potential_sql.strip()\n",
        "\n",
        "    logger.debug(\"Could not extract SQL.\")\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def extract_reasoning(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    match = re.search(r\"<reasoning>(.*?)</reasoning>\",\n",
        "                      text, re.IGNORECASE | re.DOTALL)\n",
        "    return match.group(1).strip() if match else \"\"\n",
        "\n",
        "\n",
        "def calculate_sql_complexity(sql: str) -> float:\n",
        "    if not sql:\n",
        "        return 0.0\n",
        "    try:\n",
        "        sql_upper = sql.upper()\n",
        "        score = 1.0\n",
        "        score += sql_upper.count(\" JOIN \") * 0.6\n",
        "        score += sql_upper.count(\" UNION \") * 0.8 + sql_upper.count(\n",
        "            \" INTERSECT \") * 0.8 + sql_upper.count(\" EXCEPT \") * 0.8\n",
        "        score += sql_upper.count(\"(SELECT\") * 1.0\n",
        "        score += sql_upper.count(\" WITH \") * 0.8\n",
        "        if \" WHERE \" in sql_upper:\n",
        "            score += 0.2\n",
        "        if \" GROUP BY \" in sql_upper:\n",
        "            score += 0.5\n",
        "        if \" HAVING \" in sql_upper:\n",
        "            score += 0.7\n",
        "        if \" ORDER BY \" in sql_upper:\n",
        "            score += 0.3\n",
        "        if \" LIMIT \" in sql_upper:\n",
        "            score += 0.1\n",
        "        agg_functions = [\"COUNT(\", \"SUM(\", \"AVG(\", \"MAX(\", \"MIN(\"]\n",
        "        score += sum(sql_upper.count(agg) for agg in agg_functions) * 0.3\n",
        "        score += sql_upper.count(\" DISTINCT \") * 0.3\n",
        "        score += sql_upper.count(\" CASE \") * 0.4\n",
        "        score += sql_upper.count(\" OVER(\") * 1.0\n",
        "        where_match = re.search(\n",
        "            r\" WHERE (.*?)(?: GROUP BY | ORDER BY | LIMIT | OFFSET |$)\", sql_upper, re.DOTALL)\n",
        "        if where_match:\n",
        "            where_clause = where_match.group(1)\n",
        "            score += where_clause.count(\" AND \") * \\\n",
        "                0.15 + where_clause.count(\" OR \") * 0.20\n",
        "            score += where_clause.count(\" IN \") * \\\n",
        "                0.2 + where_clause.count(\" LIKE \") * 0.1\n",
        "            score += where_clause.count(\" BETWEEN \") * \\\n",
        "                0.2 + where_clause.count(\" EXISTS \") * 0.3\n",
        "        return max(0.0, score)\n",
        "    except Exception as e:\n",
        "        logger.warning(\n",
        "            f\"Error calculating complexity for '{sql[:50]}...': {e}\")\n",
        "        return 1.0\n",
        "\n",
        "\n",
        "def identify_sql_statement_type(sql: str) -> str:\n",
        "    if not sql:\n",
        "        return \"UNKNOWN\"\n",
        "    clean_sql = re.sub(r'--.*?$', '', sql, flags=re.MULTILINE).strip()\n",
        "    clean_sql = re.sub(r'/\\*.*?\\*/', '', clean_sql, flags=re.DOTALL).strip()\n",
        "    if not clean_sql:\n",
        "        return \"UNKNOWN\"\n",
        "    first_word = clean_sql.split(None, 1)[0].upper()\n",
        "\n",
        "    if first_word == \"SELECT\":\n",
        "        return \"SELECT\"\n",
        "    if first_word == \"INSERT\":\n",
        "        return \"INSERT\"\n",
        "    if first_word == \"UPDATE\":\n",
        "        return \"UPDATE\"\n",
        "    if first_word == \"DELETE\":\n",
        "        return \"DELETE\"\n",
        "    if first_word == \"CREATE\":\n",
        "        if re.search(r\"CREATE\\s+(TABLE|VIEW|INDEX)\", clean_sql[:30], re.IGNORECASE):\n",
        "            second_word = clean_sql.split(None, 2)[1].upper() if len(\n",
        "                clean_sql.split()) > 1 else \"\"\n",
        "            if second_word == \"TABLE\":\n",
        "                return \"CREATE_TABLE\"\n",
        "            if second_word == \"VIEW\":\n",
        "                return \"CREATE_VIEW\"\n",
        "            if second_word == \"INDEX\":\n",
        "                return \"CREATE_INDEX\"\n",
        "        return \"CREATE_OTHER\"\n",
        "    if first_word == \"DROP\":\n",
        "        return \"DROP\"\n",
        "    if first_word == \"ALTER\":\n",
        "        return \"ALTER\"\n",
        "    if first_word == \"WITH\":\n",
        "        match = re.search(r'\\)\\s*(SELECT|INSERT|UPDATE|DELETE)',\n",
        "                          clean_sql, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).upper()\n",
        "        return \"WITH_UNKNOWN\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "\n",
        "def list_all_tables(conn: sqlite3.Connection) -> List[str]:\n",
        "    try:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\n",
        "            \"SELECT name FROM sqlite_master WHERE type IN ('table', 'view');\")\n",
        "        return [row[0] for row in cursor.fetchall()]\n",
        "    except sqlite3.Error as e:\n",
        "        logger.error(f\"Error listing tables/views: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def get_table_schema(conn: sqlite3.Connection, table_name: str) -> List[Tuple]:\n",
        "    try:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(f\"PRAGMA table_info('{table_name}');\")\n",
        "        return cursor.fetchall()\n",
        "    except sqlite3.Error as e:\n",
        "        logger.warning(f\"Error getting schema for table {table_name}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def check_table_exists(conn: sqlite3.Connection, table_name: str) -> Tuple[bool, bool, Optional[str]]:\n",
        "    try:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\n",
        "            \"SELECT name FROM sqlite_master WHERE type IN ('table', 'view') AND name=?;\", (table_name,))\n",
        "        exact_match = cursor.fetchone()\n",
        "        if exact_match:\n",
        "            return True, True, table_name\n",
        "\n",
        "        cursor.execute(\n",
        "            \"SELECT name FROM sqlite_master WHERE type IN ('table', 'view') AND lower(name)=lower(?);\", (table_name,))\n",
        "        insensitive_match = cursor.fetchone()\n",
        "        if insensitive_match:\n",
        "            return False, True, insensitive_match[0]\n",
        "\n",
        "        return False, False, None\n",
        "    except sqlite3.Error as e:\n",
        "        logger.warning(\n",
        "            f\"Error checking existence for table/view {table_name}: {e}\")\n",
        "        return False, False, None\n",
        "\n",
        "\n",
        "def get_column_names(conn: sqlite3.Connection, table_name: str) -> List[str]:\n",
        "    schema = get_table_schema(conn, table_name)\n",
        "    return [col[1] for col in schema]\n",
        "\n",
        "\n",
        "def extract_tables_from_query(sql: str) -> set[str]:\n",
        "    tables = set()\n",
        "    if not sql:\n",
        "        return tables\n",
        "    try:\n",
        "        tree = sqlglot.parse_one(sql, read=\"sqlite\")\n",
        "        for t in tree.find_all(exp.Table):\n",
        "            if t.name:\n",
        "                tables.add(t.name)\n",
        "    except sqlglot.errors.ParseError as e:\n",
        "        logger.warning(\n",
        "            f\"sqlglot failed to parse for table extraction: {e}. Falling back to regex.\")\n",
        "        pattern = r'(?:FROM|JOIN)\\s+([`\"\\[]?\\w+[`\"\\]]?)'\n",
        "        for match in re.finditer(pattern, sql, re.IGNORECASE):\n",
        "            tables.add(match.group(1).strip('`\"[]'))\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error during table extraction: {e}\")\n",
        "    return tables\n",
        "\n",
        "\n",
        "def convert_sql_to_sqlite(sql: str, source_dialect: str = \"mysql\") -> Optional[str]:\n",
        "    if not sql or not sql.strip():\n",
        "        return sql\n",
        "    try:\n",
        "        if DEBUG_MODE:\n",
        "            logger.debug(\n",
        "                f\"Converting SQL from {source_dialect} to sqlite: {sql[:150]}...\")\n",
        "        if source_dialect == \"postgresql\":\n",
        "            try:\n",
        "                converted = transpile(sql, read=\"postgres\", write=\"sqlite\")\n",
        "            except ParseError as pg_err:\n",
        "                logger.warning(\n",
        "                    f\"PostgreSQL parse error: {pg_err}, trying fallback conversion...\")\n",
        "                modified_sql = sql\n",
        "                modified_sql = re.sub(\n",
        "                    r'(\\w+)::\\w+', r'CAST(\\1 AS TEXT)', modified_sql)\n",
        "                modified_sql = re.sub(\n",
        "                    r'\\s+RETURNING\\s+.*?$', '', modified_sql, flags=re.IGNORECASE)\n",
        "                try:\n",
        "                    converted = transpile(\n",
        "                        modified_sql, read=\"postgres\", write=\"sqlite\")\n",
        "                except ParseError:\n",
        "                    logger.warning(\"Falling back to generic SQL parsing...\")\n",
        "                    converted = transpile(\n",
        "                        modified_sql, read=\"generic\", write=\"sqlite\")\n",
        "        else:\n",
        "            converted = transpile(sql, read=source_dialect, write=\"sqlite\")\n",
        "        if converted and isinstance(converted, list) and converted[0]:\n",
        "            if DEBUG_MODE:\n",
        "                logger.debug(f\"Converted SQL: {converted[0][:150]}...\")\n",
        "            return converted[0]\n",
        "        else:\n",
        "            logger.warning(\n",
        "                f\"sqlglot transpile returned empty result for: {sql[:100]}...\")\n",
        "            return sql\n",
        "    except ParseError as e:\n",
        "        logger.warning(\n",
        "            f\"sqlglot ParseError during conversion: {e}. SQL: {sql[:150]}...\")\n",
        "        if 'AUTO_INCREMENT' in sql.upper():\n",
        "            modified_sql = re.sub(\n",
        "                r'AUTO_INCREMENT', 'AUTOINCREMENT', sql, flags=re.IGNORECASE)\n",
        "            modified_sql = re.sub(r'(\\w+)\\s+(?:INT|INTEGER)\\s+PRIMARY\\s+KEY\\s+AUTOINCREMENT',\n",
        "                                  r'\\1 INTEGER PRIMARY KEY AUTOINCREMENT', modified_sql, flags=re.IGNORECASE)\n",
        "            logger.debug(\"Applied manual AUTO_INCREMENT fix attempt.\")\n",
        "            return modified_sql\n",
        "        return sql\n",
        "    except Exception as e:\n",
        "        logger.error(\n",
        "            f\"Unexpected error during sqlglot conversion: {e}\", exc_info=DEBUG_MODE)\n",
        "        return sql\n",
        "\n",
        "\n",
        "def fix_case_sensitivity_in_sql(conn: sqlite3.Connection, sql: str) -> str:\n",
        "    if not sql:\n",
        "        return sql\n",
        "\n",
        "    corrected_sql = sql\n",
        "    all_db_tables = list_all_tables(conn)\n",
        "    if not all_db_tables:\n",
        "        return sql\n",
        "\n",
        "    table_case_map = {t.lower(): t for t in all_db_tables}\n",
        "\n",
        "    referenced_tables = extract_tables_from_query(corrected_sql)\n",
        "    needs_table_fix = False\n",
        "    for table in referenced_tables:\n",
        "        table_lower = table.lower()\n",
        "        if table not in table_case_map.values() and table_lower in table_case_map:\n",
        "            correct_case_table = table_case_map[table_lower]\n",
        "            logger.debug(\n",
        "                f\"Case Fix: Replacing table '{table}' with '{correct_case_table}'\")\n",
        "            corrected_sql = re.sub(r'\\b' + re.escape(table) + r'\\b',\n",
        "                                   correct_case_table, corrected_sql, flags=re.IGNORECASE)\n",
        "            needs_table_fix = True\n",
        "\n",
        "    if needs_table_fix:\n",
        "        logger.debug(\n",
        "            f\"SQL after table case correction: {corrected_sql[:150]}...\")\n",
        "\n",
        "    current_referenced_tables = extract_tables_from_query(corrected_sql)\n",
        "    needs_column_fix = False\n",
        "\n",
        "    try:\n",
        "        parsed_exp = sqlglot_parse(corrected_sql, read=\"sqlite\")\n",
        "\n",
        "        if isinstance(parsed_exp, list):\n",
        "            all_col_refs = []\n",
        "            for expr in parsed_exp:\n",
        "                if hasattr(expr, 'find_all'):\n",
        "                    all_col_refs.extend(expr.find_all(Column))\n",
        "        else:\n",
        "            all_col_refs = parsed_exp.find_all(Column)\n",
        "\n",
        "        for col_exp in all_col_refs:\n",
        "            col_name = col_exp.name\n",
        "            table_alias_or_name = col_exp.table\n",
        "\n",
        "            target_table = None\n",
        "            if table_alias_or_name:\n",
        "                if table_alias_or_name.lower() in table_case_map:\n",
        "                    target_table = table_case_map[table_alias_or_name.lower()]\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "            if target_table:\n",
        "                db_columns = get_column_names(conn, target_table)\n",
        "                col_case_map = {c.lower(): c for c in db_columns}\n",
        "                if col_name not in db_columns and col_name.lower() in col_case_map:\n",
        "                    correct_case_col = col_case_map[col_name.lower()]\n",
        "                    logger.debug(\n",
        "                        f\"Case Fix: Replacing column '{table_alias_or_name}.{col_name}' with '{table_alias_or_name}.{correct_case_col}'\")\n",
        "                    pattern = r'\\b' + \\\n",
        "                        re.escape(table_alias_or_name) + \\\n",
        "                        r'\\s*\\.\\s*' + re.escape(col_name) + r'\\b'\n",
        "                    replacement = f\"{table_alias_or_name}.{correct_case_col}\"\n",
        "                    corrected_sql = re.sub(\n",
        "                        pattern, replacement, corrected_sql, flags=re.IGNORECASE)\n",
        "                    needs_column_fix = True\n",
        "\n",
        "            elif not table_alias_or_name:\n",
        "                possible_corrections = []\n",
        "                for ref_table_name_lower in current_referenced_tables:\n",
        "                    actual_ref_table = table_case_map.get(\n",
        "                        ref_table_name_lower.lower())\n",
        "                    if actual_ref_table:\n",
        "                        db_columns = get_column_names(conn, actual_ref_table)\n",
        "                        col_case_map = {c.lower(): c for c in db_columns}\n",
        "                        if col_name not in db_columns and col_name.lower() in col_case_map:\n",
        "                            possible_corrections.append(\n",
        "                                col_case_map[col_name.lower()])\n",
        "\n",
        "                if len(possible_corrections) == 1:\n",
        "                    correct_case_col = possible_corrections[0]\n",
        "                    logger.debug(\n",
        "                        f\"Case Fix: Replacing unqualified column '{col_name}' with '{correct_case_col}'\")\n",
        "                    pattern = r'(?<![\\w\\.])\\b' + re.escape(col_name) + r'\\b'\n",
        "                    corrected_sql = re.sub(\n",
        "                        pattern, correct_case_col, corrected_sql, flags=re.IGNORECASE)\n",
        "                    needs_column_fix = True\n",
        "                elif len(possible_corrections) > 1:\n",
        "                    logger.warning(\n",
        "                        f\"Ambiguous case correction for unqualified column '{col_name}'. Found in multiple tables. Skipping.\")\n",
        "\n",
        "    except ParseError as e:\n",
        "        logger.warning(\n",
        "            f\"sqlglot failed to parse for column case fixing: {e}. Column fix might be incomplete.\")\n",
        "    except Exception as e:\n",
        "        logger.error(\n",
        "            f\"Unexpected error during column case fixing: {e}\", exc_info=DEBUG_MODE)\n",
        "\n",
        "    if needs_column_fix:\n",
        "        logger.debug(\n",
        "            f\"SQL after column case correction: {corrected_sql[:150]}...\")\n",
        "\n",
        "    return corrected_sql\n",
        "\n",
        "\n",
        "def fix_ambiguous_columns(sql: str, conn: Optional[sqlite3.Connection] = None) -> str:\n",
        "    if \" JOIN \" not in sql.upper():\n",
        "        return sql\n",
        "\n",
        "    try:\n",
        "        parsed_exp = sqlglot_parse(sql, read=\"sqlite\")\n",
        "        common_ambiguous = {'id', 'name', 'date', 'code', 'created_at', 'updated_at',\n",
        "                            'description', 'status', 'type', 'price', 'quantity', 'amount'}\n",
        "        first_table_alias = None\n",
        "\n",
        "        if isinstance(parsed_exp, list):\n",
        "            tables = []\n",
        "            for expr in parsed_exp:\n",
        "                if hasattr(expr, 'find_all'):\n",
        "                    for node in expr.find_all():\n",
        "                        if hasattr(node, 'name') and hasattr(node, 'is_table') and node.is_table:\n",
        "                            tables.append(node)\n",
        "        else:\n",
        "            tables = [node for node in parsed_exp.find_all()\n",
        "                      if hasattr(node, 'name') and hasattr(node, 'is_table') and node.is_table]\n",
        "\n",
        "        if tables:\n",
        "            first_table_alias = tables[0].alias_or_name\n",
        "\n",
        "        if not first_table_alias:\n",
        "            return sql\n",
        "\n",
        "        fixed_sql = sql\n",
        "        modified = False\n",
        "\n",
        "        if isinstance(parsed_exp, list):\n",
        "            all_col_refs = []\n",
        "            for expr in parsed_exp:\n",
        "                if hasattr(expr, 'find_all'):\n",
        "                    all_col_refs.extend(expr.find_all(Column))\n",
        "        else:\n",
        "            all_col_refs = parsed_exp.find_all(Column)\n",
        "\n",
        "        for col_exp in all_col_refs:\n",
        "            if not col_exp.table and col_exp.name.lower() in common_ambiguous:\n",
        "                logger.debug(\n",
        "                    f\"Ambiguity Fix: Qualifying '{col_exp.name}' with '{first_table_alias}'\")\n",
        "                pattern = r'(?<![\\w\\.])\\b' + re.escape(col_exp.name) + r'\\b'\n",
        "                replacement = f\"{first_table_alias}.{col_exp.name}\"\n",
        "                fixed_sql = re.sub(pattern, replacement,\n",
        "                                   fixed_sql, flags=re.IGNORECASE)\n",
        "                modified = True\n",
        "\n",
        "        if modified:\n",
        "            logger.debug(\n",
        "                f\"SQL after ambiguity fix attempt: {fixed_sql[:150]}...\")\n",
        "            return fixed_sql\n",
        "        else:\n",
        "            return sql\n",
        "\n",
        "    except ParseError:\n",
        "        logger.warning(\n",
        "            \"Failed to parse SQL for ambiguity fixing. Returning original.\")\n",
        "        return sql\n",
        "    except Exception as e:\n",
        "        logger.error(\n",
        "            f\"Error during ambiguity fixing: {e}\", exc_info=DEBUG_MODE)\n",
        "        return sql\n",
        "\n",
        "\n",
        "def categorize_sql_error(error_msg: str) -> Tuple[str, float]:\n",
        "    if not error_msg:\n",
        "        return ERR_OTHER, 0.0\n",
        "    error_lower = error_msg.lower()\n",
        "    if DEBUG_MODE:\n",
        "        logger.debug(f\"Categorizing SQL error: {error_msg}\")\n",
        "\n",
        "    if \"syntax error\" in error_lower:\n",
        "        return ERR_SYNTAX, 0.0\n",
        "    if \"no such table\" in error_lower:\n",
        "        return ERR_MISSING_TABLE, 0.0\n",
        "    if \"no such column\" in error_lower:\n",
        "        return ERR_MISSING_COLUMN, 0.1\n",
        "    if \"ambiguous column\" in error_lower:\n",
        "        return ERR_AMBIGUOUS_COLUMN, 0.2\n",
        "    if \"datatype mismatch\" in error_lower:\n",
        "        return ERR_TYPE_MISMATCH, 0.15\n",
        "    if \"constraint failed\" in error_lower or \"constraint violation\" in error_lower:\n",
        "        return ERR_CONSTRAINT, 0.1\n",
        "    if \"no such function\" in error_lower:\n",
        "        return ERR_FUNCTION, 0.05\n",
        "    if \"too many terms in compound select\" in error_lower:\n",
        "        return ERR_SYNTAX, 0.0\n",
        "    if \"subquery returned more than 1 row\" in error_lower:\n",
        "        return ERR_EXECUTION, 0.1\n",
        "\n",
        "    return ERR_OTHER, 0.0\n",
        "\n",
        "\n",
        "def extract_tables_columns(sql_context: str) -> tuple[set[str], set[str]]:\n",
        "    tables = set()\n",
        "    columns = set()\n",
        "    if not sql_context:\n",
        "        return tables, columns\n",
        "\n",
        "    create_table_pattern = r\"CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?(?:[`\\\"\\[]?(\\w+)[`\\\"\\]]?)\\s*\\((.*?)\\);\"\n",
        "    create_view_pattern = r\"CREATE\\s+VIEW\\s+(?:[`\\\"\\[]?(\\w+)[`\\\"\\]]?)\\s+AS\"\n",
        "    column_pattern = r\"^\\s*([`\\\"\\[]?\\w+[`\\\"\\]]?)\"\n",
        "\n",
        "    try:\n",
        "        statements = sqlparse.split(sql_context)\n",
        "        for stmt in statements:\n",
        "            stmt_clean = stmt.strip()\n",
        "            table_match = re.search(\n",
        "                create_table_pattern, stmt_clean, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
        "            if table_match:\n",
        "                table_name = table_match.group(1).lower()\n",
        "                tables.add(table_name)\n",
        "                cols_text = table_match.group(2)\n",
        "                for part in re.split(r',(?![^\\(]*\\))', cols_text):\n",
        "                    col_match = re.match(column_pattern, part.strip())\n",
        "                    if col_match:\n",
        "                        columns.add(col_match.group(1).strip('`\"[]').lower())\n",
        "            view_match = re.search(create_view_pattern,\n",
        "                                   stmt_clean, re.IGNORECASE)\n",
        "            if view_match:\n",
        "                view_name = view_match.group(1).lower()\n",
        "                tables.add(view_name)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not parse schema elements from context: {e}\")\n",
        "\n",
        "    return tables, columns\n",
        "\n",
        "\n",
        "def reasoning_quality_reward(prompts, completions, references=None, **kwargs) -> list[float]:\n",
        "    rewards = []\n",
        "    schema_cache = {}\n",
        "\n",
        "    for i, completion in enumerate(completions):\n",
        "        response_text = _get_response_text(completion)\n",
        "        reasoning = extract_reasoning(response_text)\n",
        "        reward_components = {}\n",
        "\n",
        "        if not reasoning:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "\n",
        "        reasoning_lower = reasoning.lower()\n",
        "        words = reasoning.split()\n",
        "        lines = [line for line in reasoning.split(\"\\n\") if line.strip()]\n",
        "\n",
        "        len_score = 0.0\n",
        "        if len(words) >= 50:\n",
        "            len_score = 0.20\n",
        "        elif len(words) >= 25:\n",
        "            len_score = 0.15\n",
        "        elif len(words) >= 10:\n",
        "            len_score = 0.10\n",
        "        reward_components['length'] = len_score\n",
        "\n",
        "        sql_terms = [\"table\", \"column\", \"join\", \"select\", \"where\",\n",
        "                     \"group by\", \"order by\", \"filter\", \"aggregate\", \"schema\", \"database\"]\n",
        "        term_count = sum(1 for term in sql_terms if term in reasoning_lower)\n",
        "        term_score = min(0.20, term_count * 0.03)\n",
        "        reward_components['terms'] = term_score\n",
        "\n",
        "        structure_score = 0.0\n",
        "        if len(lines) >= 3:\n",
        "            structure_score = 0.15\n",
        "        elif len(lines) >= 2:\n",
        "            structure_score = 0.10\n",
        "        reward_components['structure'] = structure_score\n",
        "\n",
        "        step_score = 0.0\n",
        "        if re.search(r'(step 1|first|start|initial|begin)', reasoning_lower) and \\\n",
        "           re.search(r'(step 2|next|then|second|final|last|subsequent)', reasoning_lower):\n",
        "            step_score = 0.15\n",
        "        reward_components['steps'] = step_score\n",
        "\n",
        "        schema_mention_score = 0.0\n",
        "        sql_context = _sql_ref(references, i).get('sql_context')\n",
        "        if sql_context:\n",
        "            if i not in schema_cache:\n",
        "                schema_cache[i] = extract_tables_columns(\n",
        "                    sql_context) if isinstance(sql_context, str) else (set(), set())\n",
        "            tables, columns = schema_cache[i]\n",
        "\n",
        "            if tables or columns:\n",
        "                mentioned_tables = sum(1 for t in tables if re.search(\n",
        "                    r'\\b' + re.escape(t) + r'\\b', reasoning_lower))\n",
        "                mentioned_cols = sum(1 for c in columns if re.search(\n",
        "                    r'\\b' + re.escape(c) + r'\\b', reasoning_lower))\n",
        "                total_mentions = mentioned_tables + mentioned_cols\n",
        "                schema_mention_score = min(0.30, total_mentions * 0.05)\n",
        "        reward_components['schema'] = schema_mention_score\n",
        "\n",
        "        total_unscaled_reward = sum(reward_components.values())\n",
        "        final_reward = min(1.0, total_unscaled_reward) * \\\n",
        "            REWARD_WEIGHTS.get(\"reasoning\", 0.7)\n",
        "        rewards.append(final_reward)\n",
        "        if DEBUG_MODE:\n",
        "            logger.debug(\n",
        "                f\"Reasoning Scores (Comp {i}): {reward_components} -> Total Raw: {total_unscaled_reward:.3f} -> Final: {final_reward:.3f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def complexity_reward(prompts, completions, references, **kwargs) -> list[float]:\n",
        "    rewards = []\n",
        "    base_weight = REWARD_WEIGHTS.get(\"complexity\", 0.6)\n",
        "\n",
        "    for i, completion in enumerate(completions):\n",
        "        response_text = _get_response_text(completion)\n",
        "\n",
        "        gen_sql = extract_sql(response_text)\n",
        "        if not gen_sql:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "\n",
        "        gold_sql = _sql_ref(references, i).get('gold_sql', '')\n",
        "        if not gen_sql:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            gen_complexity = calculate_sql_complexity(gen_sql)\n",
        "\n",
        "            if not gold_sql:\n",
        "                reward = (0.4 if 1.5 <= gen_complexity <=\n",
        "                          8.0 else 0.1) * base_weight\n",
        "                if DEBUG_MODE:\n",
        "                    logger.debug(\n",
        "                        f\"Complexity (Comp {i}): No Gold SQL. Gen={gen_complexity:.2f}. Reward={reward:.3f}\")\n",
        "            else:\n",
        "                gold_complexity = calculate_sql_complexity(gold_sql)\n",
        "                if gold_complexity < 0.1:\n",
        "                    rel_score = 1.0 if gen_complexity < 0.1 else 0.0\n",
        "                else:\n",
        "                    ratio = max(\n",
        "                        1e-3, min(gen_complexity / gold_complexity, 1e3))\n",
        "                    log_ratio = torch.log(torch.tensor(ratio))\n",
        "                    rel_score = torch.exp(-0.5 * (log_ratio**2)).item()\n",
        "                reward = rel_score * base_weight\n",
        "                if DEBUG_MODE:\n",
        "                    logger.debug(\n",
        "                        f\"Complexity (Comp {i}): Gen={gen_complexity:.2f}, Gold={gold_complexity:.2f}, Ratio={ratio:.2f}, Score={rel_score:.3f}, Reward={reward:.3f}\")\n",
        "\n",
        "            rewards.append(max(0.0, reward))\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in complexity reward calculation: {e}\")\n",
        "            rewards.append(0.0)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def dump_database_schema(conn):\n",
        "    try:\n",
        "        cursor = conn.cursor()\n",
        "        tables = list_all_tables(conn)\n",
        "\n",
        "        schema_info = {}\n",
        "\n",
        "        for table in tables:\n",
        "            cursor.execute(f\"PRAGMA table_info({table})\")\n",
        "            columns = cursor.fetchall()\n",
        "\n",
        "            column_info = []\n",
        "            for col in columns:\n",
        "                col_id, name, col_type, not_null, default_val, is_pk = col\n",
        "                col_desc = f\"{name} ({col_type})\"\n",
        "                if is_pk:\n",
        "                    col_desc += \" PRIMARY KEY\"\n",
        "                if not_null:\n",
        "                    col_desc += \" NOT NULL\"\n",
        "                if default_val is not None:\n",
        "                    col_desc += f\" DEFAULT {default_val}\"\n",
        "                column_info.append(col_desc)\n",
        "\n",
        "            schema_info[table] = column_info\n",
        "\n",
        "            cursor.execute(f\"PRAGMA index_list({table})\")\n",
        "            indexes = cursor.fetchall()\n",
        "            if indexes:\n",
        "                schema_info[f\"{table}_indexes\"] = []\n",
        "                for idx in indexes:\n",
        "                    idx_name = idx[1]\n",
        "                    cursor.execute(f\"PRAGMA index_info({idx_name})\")\n",
        "                    idx_columns = cursor.fetchall()\n",
        "                    idx_cols = [info[2] for info in idx_columns]\n",
        "                    schema_info[f\"{table}_indexes\"].append(\n",
        "                        f\"{idx_name} ({', '.join(idx_cols)})\")\n",
        "\n",
        "        return schema_info\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error dumping database schema: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "\n",
        "def execute_query_reward_func(prompts, completions, references, **kwargs) -> list[float]:\n",
        "    rewards = []\n",
        "\n",
        "    for i, completion in enumerate(completions):\n",
        "        response_text = _get_response_text(completion)\n",
        "        gen_sql = extract_sql(response_text)\n",
        "\n",
        "        ref_i = _sql_ref(references, i)\n",
        "        gold_sql = ref_i.get('gold_sql', '')\n",
        "        sql_context = ref_i.get('sql_context', '')\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            logger.debug(f\"Reference {i}: Gold SQL = {gold_sql[:100]}...\")\n",
        "            logger.debug(f\"Reference {i}: Context SQL  = {sql_context}\")\n",
        "        reward = 0.0\n",
        "\n",
        "        if not gen_sql or not gold_sql or not sql_context:\n",
        "            logger.warning(\n",
        "                f\"Missing SQL data for completion {i}: gen_sql={bool(gen_sql)}, gold_sql={bool(gold_sql)}, sql_context={bool(sql_context)}\")\n",
        "            rewards.append(reward)\n",
        "            continue\n",
        "\n",
        "        gold_type = identify_sql_statement_type(gold_sql)\n",
        "        gen_type = identify_sql_statement_type(gen_sql)\n",
        "\n",
        "        type_match = (gold_type == gen_type)\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            logger.debug(f\"Gold SQL type: {gold_type}\")\n",
        "            logger.debug(f\"Generated SQL type: {gen_type}\")\n",
        "\n",
        "        conn = None\n",
        "        temp_db_file = None\n",
        "        try:\n",
        "            temp_db_file = tempfile.NamedTemporaryFile(delete=False).name\n",
        "            conn = sqlite3.connect(temp_db_file, timeout=5)\n",
        "            conn.isolation_level = None\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            create_table_statements = []\n",
        "            create_view_statements = []\n",
        "            other_statements = []\n",
        "\n",
        "            for stmt in sqlparse.split(sql_context):\n",
        "                stmt = stmt.strip()\n",
        "                if not stmt:\n",
        "                    continue\n",
        "\n",
        "                stmt_upper = stmt.upper()\n",
        "                if stmt_upper.startswith('CREATE TABLE'):\n",
        "                    create_table_statements.append(stmt)\n",
        "                elif stmt_upper.startswith('CREATE VIEW'):\n",
        "                    create_view_statements.append(stmt)\n",
        "                else:\n",
        "                    other_statements.append(stmt)\n",
        "\n",
        "            if DEBUG_MODE:\n",
        "                logger.debug(f\"Found {len(create_table_statements)} CREATE TABLE statements, \"\n",
        "                             f\"{len(create_view_statements)} CREATE VIEW statements, and \"\n",
        "                             f\"{len(other_statements)} other statements\")\n",
        "\n",
        "            tables_created = []\n",
        "            for stmt in create_table_statements:\n",
        "                try:\n",
        "                    table_match = re.search(r'CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?([^\\s(]+)',\n",
        "                                            stmt, re.IGNORECASE)\n",
        "                    table_name = table_match.group(1).strip(\n",
        "                        '`\"[]') if table_match else \"unknown\"\n",
        "\n",
        "                    converted_stmt = convert_sql_to_sqlite(stmt)\n",
        "\n",
        "                    if DEBUG_MODE:\n",
        "                        logger.debug(\n",
        "                            f\"Creating table {table_name} with statement: {converted_stmt[:100]}...\")\n",
        "\n",
        "                    cursor.execute(converted_stmt)\n",
        "                    tables_created.append(table_name)\n",
        "\n",
        "                    exists_exact, exists_case_insensitive, correct_case = check_table_exists(\n",
        "                        conn, table_name)\n",
        "                    if exists_exact:\n",
        "                        if DEBUG_MODE:\n",
        "                            logger.debug(\n",
        "                                f\"Table {table_name} created successfully\")\n",
        "                            schema = get_table_schema(conn, table_name)\n",
        "                            logger.debug(f\"Schema for {table_name}: {schema}\")\n",
        "                    else:\n",
        "                        logger.warning(\n",
        "                            f\"Table {table_name} creation failed silently\")\n",
        "\n",
        "                except sqlite3.Error as e:\n",
        "                    logger.warning(f\"Error in CREATE TABLE statement: {e}\")\n",
        "                    logger.warning(\n",
        "                        f\"Table name: {table_name if 'table_name' in locals() else 'unknown'}\")\n",
        "                    logger.warning(f\"Original statement: {stmt[:200]}...\")\n",
        "                    logger.warning(f\"Converted statement: {converted_stmt[:200]}...\" if 'converted_stmt' in locals(\n",
        "                    ) else \"conversion failed\")\n",
        "\n",
        "            views_created = []\n",
        "            for stmt in create_view_statements:\n",
        "                try:\n",
        "                    view_match = re.search(\n",
        "                        r'CREATE\\s+VIEW\\s+([^\\s(]+)', stmt, re.IGNORECASE)\n",
        "                    view_name = view_match.group(1).strip(\n",
        "                        '`\"[]') if view_match else \"unknown\"\n",
        "\n",
        "                    converted_stmt = convert_sql_to_sqlite(stmt)\n",
        "\n",
        "                    if DEBUG_MODE:\n",
        "                        logger.debug(\n",
        "                            f\"Creating view {view_name} with statement: {converted_stmt[:100]}...\")\n",
        "\n",
        "                    cursor.execute(converted_stmt)\n",
        "                    views_created.append(view_name)\n",
        "\n",
        "                    exists_exact, exists_case_insensitive, correct_case = check_table_exists(\n",
        "                        conn, view_name)\n",
        "                    if exists_exact:\n",
        "                        if DEBUG_MODE:\n",
        "                            logger.debug(\n",
        "                                f\"View {view_name} created successfully\")\n",
        "                    else:\n",
        "                        logger.warning(\n",
        "                            f\"View {view_name} creation failed silently\")\n",
        "\n",
        "                except sqlite3.Error as e:\n",
        "                    logger.warning(f\"Error in CREATE VIEW statement: {e}\")\n",
        "                    logger.warning(\n",
        "                        f\"View name: {view_name if 'view_name' in locals() else 'unknown'}\")\n",
        "                    logger.warning(f\"Original statement: {stmt[:200]}...\")\n",
        "                    logger.warning(f\"Converted statement: {converted_stmt[:200]}...\" if 'converted_stmt' in locals(\n",
        "                    ) else \"conversion failed\")\n",
        "\n",
        "            for stmt in other_statements:\n",
        "                try:\n",
        "                    is_insert_like = stmt.upper().startswith(\n",
        "                        \"INSERT\") or \"INSERT INTO\" in stmt.upper()\n",
        "\n",
        "                    converted_stmt = convert_sql_to_sqlite(stmt)\n",
        "\n",
        "                    if DEBUG_MODE and is_insert_like:\n",
        "                        logger.debug(\n",
        "                            f\"Executing insert-like statement: {converted_stmt[:100]}...\")\n",
        "\n",
        "                    cursor.execute(converted_stmt)\n",
        "                except sqlite3.Error as e:\n",
        "                    logger.warning(f\"Error in non-CREATE statement: {e}\")\n",
        "                    logger.warning(f\"Statement causing error: {stmt[:200]}...\")\n",
        "\n",
        "            if DEBUG_MODE:\n",
        "                schema_info = dump_database_schema(conn)\n",
        "                logger.debug(f\"Database schema after setup: {schema_info}\")\n",
        "\n",
        "                all_tables = list_all_tables(conn)\n",
        "                logger.debug(f\"All tables in database: {all_tables}\")\n",
        "\n",
        "            referenced_tables = extract_tables_from_query(gen_sql)\n",
        "            if DEBUG_MODE:\n",
        "                logger.debug(\n",
        "                    f\"Tables referenced in generated query: {referenced_tables}\")\n",
        "\n",
        "                for table in referenced_tables:\n",
        "                    exists_exact, exists_case_insensitive, correct_case = check_table_exists(\n",
        "                        conn, table)\n",
        "                    if exists_exact:\n",
        "                        logger.debug(\n",
        "                            f\"Table '{table}' referenced in query exists exactly as specified\")\n",
        "                    elif exists_case_insensitive:\n",
        "                        logger.debug(\n",
        "                            f\"Table '{table}' exists but with different case: '{correct_case}'\")\n",
        "                    else:\n",
        "                        logger.debug(\n",
        "                            f\"Table '{table}' does not exist in any case form\")\n",
        "\n",
        "            existing_tables = list_all_tables(conn)\n",
        "            existing_tables_lower = [t.lower() for t in existing_tables]\n",
        "            missing_tables = [table for table in referenced_tables if table.lower(\n",
        "            ) not in existing_tables_lower]\n",
        "            case_mismatch_tables = [\n",
        "                table for table in referenced_tables if table not in existing_tables and table.lower() in existing_tables_lower]\n",
        "\n",
        "            if case_mismatch_tables:\n",
        "                logger.warning(\n",
        "                    f\"Case-mismatch in table references: {case_mismatch_tables}\")\n",
        "\n",
        "                case_mapping = {t.lower(): t for t in existing_tables}\n",
        "\n",
        "                for wrong_case in case_mismatch_tables:\n",
        "                    correct_case = case_mapping[wrong_case.lower()]\n",
        "                    logger.debug(\n",
        "                        f\"Fixing case: '{wrong_case}' → '{correct_case}'\")\n",
        "\n",
        "                    gen_sql = re.sub(r'\\b' + re.escape(wrong_case) + r'\\b',\n",
        "                                     correct_case,\n",
        "                                     gen_sql,\n",
        "                                     flags=re.IGNORECASE)\n",
        "\n",
        "                logger.debug(\n",
        "                    f\"Adjusted SQL with correct case: {gen_sql[:200]}...\")\n",
        "\n",
        "            if missing_tables:\n",
        "                logger.warning(\n",
        "                    f\"Tables genuinely missing (not just case mismatch): {missing_tables}\")\n",
        "\n",
        "            if gold_type == \"SELECT\" and gen_type == \"SELECT\":\n",
        "                try:\n",
        "                    fixed_gen_sql = fix_ambiguous_columns(gen_sql)\n",
        "\n",
        "                    if fixed_gen_sql != gen_sql:\n",
        "                        logger.debug(\n",
        "                            f\"Fixed ambiguous columns in generated SQL\")\n",
        "                        logger.debug(f\"Original SQL: {gen_sql[:200]}...\")\n",
        "                        logger.debug(f\"Fixed SQL: {fixed_gen_sql[:200]}...\")\n",
        "                        gen_sql = fixed_gen_sql\n",
        "\n",
        "                    converted_gold_sql = convert_sql_to_sqlite(gold_sql)\n",
        "                    logger.debug(\n",
        "                        f\"Executing gold SQL: {converted_gold_sql[:200]}...\")\n",
        "\n",
        "                    cursor.execute(converted_gold_sql)\n",
        "                    gold_columns = [\n",
        "                        desc[0] for desc in cursor.description] if cursor.description else []\n",
        "                    gold_result = cursor.fetchmany(1000)\n",
        "\n",
        "                    logger.debug(\n",
        "                        f\"Gold SQL execution successful, returned {len(gold_result)} rows\")\n",
        "                    if gold_result and len(gold_result) > 0:\n",
        "                        logger.debug(\n",
        "                            f\"First row of gold result: {gold_result[0]}\")\n",
        "\n",
        "                    gen_sql_fixed = fix_case_sensitivity_in_sql(conn, gen_sql)\n",
        "\n",
        "                    if gen_sql_fixed != gen_sql:\n",
        "                        logger.debug(\n",
        "                            f\"Fixed case sensitivity issues in generated SQL\")\n",
        "                        gen_sql = gen_sql_fixed\n",
        "\n",
        "                    converted_gen_sql = convert_sql_to_sqlite(gen_sql)\n",
        "                    logger.debug(\n",
        "                        f\"Executing generated SQL: {converted_gen_sql[:200]}...\")\n",
        "\n",
        "                    cursor.execute(converted_gen_sql)\n",
        "                    gen_columns = [\n",
        "                        desc[0] for desc in cursor.description] if cursor.description else []\n",
        "                    gen_result = cursor.fetchmany(1000)\n",
        "\n",
        "                    logger.debug(\n",
        "                        f\"Generated SQL execution successful, returned {len(gen_result)} rows\")\n",
        "                    if gen_result and len(gen_result) > 0:\n",
        "                        logger.debug(\n",
        "                            f\"First row of generated result: {gen_result[0]}\")\n",
        "\n",
        "                    base_reward = (0.3 + (0.1 if type_match else 0.0)) * REWARD_WEIGHTS[\"sql_correctness\"]\n",
        "                    reward = base_reward\n",
        "\n",
        "                    gold_rows = set(tuple(row) for row in gold_result)\n",
        "                    gen_rows = set(tuple(row) for row in gen_result)\n",
        "                    gold_sig = _select_signature_ignore_aliases(converted_gold_sql)\n",
        "                    gen_sig  = _select_signature_ignore_aliases(converted_gen_sql)\n",
        "                    intersection = 0\n",
        "                    jaccard = 0.0\n",
        "                    # 1) Полное совпадение по данным + совпадение проекции (без алиасов)\n",
        "                    if gold_rows == gen_rows and gold_sig and gen_sig and gold_sig == gen_sig:\n",
        "                        reward = REWARD_WEIGHTS[\"sql_correctness\"]\n",
        "                        logger.debug(\"Exact match: rows match and SELECT signature matches (aliases ignored).\")\n",
        "                    # 2) Фоллбек: если sqlglot не смог распарсить — не наказываем за алиасы\n",
        "                    elif gold_rows == gen_rows and len(gold_columns) == len(gen_columns):\n",
        "                        reward = REWARD_WEIGHTS[\"sql_correctness\"]\n",
        "                        logger.debug(\"Exact match fallback: rows match and column count matches (ignoring output labels).\")\n",
        "                    elif gold_rows and gen_rows:\n",
        "                        if gold_columns == gen_columns:\n",
        "                            intersection = len(\n",
        "                                gold_rows.intersection(gen_rows))\n",
        "                            union = len(gold_rows.union(gen_rows))\n",
        "                            jaccard = intersection / union if union > 0 else 0\n",
        "                        else:\n",
        "                            gold_cols_lower = [c.lower() for c in gold_columns]\n",
        "                            gen_cols_lower = [c.lower() for c in gen_columns]\n",
        "                            common_columns_indices = []\n",
        "                            if gold_sig and gen_sig:\n",
        "                                common_columns_indices = _match_positions_by_signature(gold_sig, gen_sig)\n",
        "\n",
        "                            # если не получилось (например, sqlglot не распарсил) — fallback на имена\n",
        "                            if not common_columns_indices:\n",
        "                                gold_cols_lower = [c.lower() for c in gold_columns]\n",
        "                                gen_cols_lower = [c.lower() for c in gen_columns]\n",
        "                                for gi, gold_col in enumerate(gold_cols_lower):\n",
        "                                    if gold_col in gen_cols_lower:\n",
        "                                        gj = gen_cols_lower.index(gold_col)\n",
        "                                        common_columns_indices.append((gi, gj))\n",
        "\n",
        "                            if common_columns_indices:\n",
        "                                # фиксированный порядок: как в common_columns_indices\n",
        "                                gold_proj_rows = {\n",
        "                                    tuple(row[gi] for gi, _ in common_columns_indices)\n",
        "                                    for row in gold_result\n",
        "                                }\n",
        "                                gen_proj_rows = {\n",
        "                                    tuple(row[gj] for _, gj in common_columns_indices)\n",
        "                                    for row in gen_result\n",
        "                                }\n",
        "\n",
        "                                intersection = len(gold_proj_rows.intersection(gen_proj_rows))\n",
        "                                union = len(gold_proj_rows.union(gen_proj_rows))\n",
        "                                jaccard = intersection / union if union > 0 else 0.0\n",
        "\n",
        "                                if DEBUG_MODE:\n",
        "                                    logger.debug(f\"Similarity calculated on {len(common_columns_indices)} common columns\")\n",
        "                            else:\n",
        "                                jaccard = 0.0\n",
        "                                intersection = 0\n",
        "\n",
        "\n",
        "                        row_count_ratio = min(len(gen_rows), len(gold_rows)) / max(\n",
        "                            len(gen_rows), len(gold_rows)) if max(len(gen_rows), len(gold_rows)) > 0 else 0\n",
        "\n",
        "                        col_similarity = 0.0\n",
        "                        if gold_sig and gen_sig:\n",
        "                            gold_cols_set = set(gold_sig)\n",
        "                            gen_cols_set  = set(gen_sig)\n",
        "                        elif gold_columns and gen_columns:\n",
        "                            gold_cols_set = set(c.lower() for c in gold_columns)\n",
        "                            gen_cols_set  = set(c.lower() for c in gen_columns)\n",
        "                        else:\n",
        "                            gold_cols_set = set()\n",
        "                            gen_cols_set = set()\n",
        "\n",
        "                        if gold_cols_set or gen_cols_set:\n",
        "                            col_intersection = len(gold_cols_set.intersection(gen_cols_set))\n",
        "                            col_union = len(gold_cols_set.union(gen_cols_set))\n",
        "                            col_similarity = col_intersection / col_union if col_union > 0 else 0.0\n",
        "\n",
        "\n",
        "                        data_accuracy = len(gold_rows.intersection(\n",
        "                            gen_rows)) / len(gold_rows) if gold_rows else 0\n",
        "\n",
        "                        content_similarity = (\n",
        "                            0.40 * jaccard +\n",
        "                            0.20 * row_count_ratio +\n",
        "                            0.25 * col_similarity +\n",
        "                            0.15 * data_accuracy\n",
        "                        )\n",
        "\n",
        "                        reward = REWARD_WEIGHTS[\"sql_correctness\"] * \\\n",
        "                            content_similarity\n",
        "\n",
        "                        if DEBUG_MODE:\n",
        "                            logger.debug(f\"Reward calculation: jaccard={jaccard:.3f}, row_ratio={row_count_ratio:.3f}, \" +\n",
        "                                         f\"col_sim={col_similarity:.3f}, data_acc={data_accuracy:.3f}, \" +\n",
        "                                         f\"content_sim={content_similarity:.3f}, final_reward={reward:.3f}\")\n",
        "\n",
        "                        if intersection > 0 and reward < 0.3 * REWARD_WEIGHTS[\"sql_correctness\"]:\n",
        "                            reward = 0.3 * REWARD_WEIGHTS[\"sql_correctness\"]\n",
        "\n",
        "                    if reward <= base_reward and gen_result is not None:\n",
        "                        reward = max(\n",
        "                            reward, 0.2 * REWARD_WEIGHTS[\"sql_correctness\"])\n",
        "\n",
        "                except sqlite3.Error as e:\n",
        "                    error_msg = str(e)\n",
        "                    error_type, partial_credit = categorize_sql_error(\n",
        "                        error_msg)\n",
        "\n",
        "                    if partial_credit > 0:\n",
        "                        reward = partial_credit * \\\n",
        "                            REWARD_WEIGHTS[\"sql_correctness\"]\n",
        "\n",
        "                    logger.warning(\n",
        "                        f\"Error executing SELECT statement ({error_type}): {error_msg}\")\n",
        "                    logger.warning(f\"Generated SQL: {gen_sql[:200]}...\")\n",
        "                    if 'converted_gen_sql' in locals():\n",
        "                        logger.warning(\n",
        "                            f\"Converted SQL: {converted_gen_sql[:200]}...\")\n",
        "\n",
        "            elif gen_type in [\"INSERT\", \"UPDATE\", \"DELETE\"]:\n",
        "                try:\n",
        "                    if \"JOIN\" in gen_sql.upper() and gen_type != \"SELECT\":\n",
        "                        logger.warning(\n",
        "                            f\"JOIN detected in {gen_type} statement - may cause issues\")\n",
        "\n",
        "                        if gen_type == \"INSERT\":\n",
        "                            table_match = re.search(\n",
        "                                r'INSERT\\s+INTO\\s+([^\\s(]+)', gen_sql, re.IGNORECASE)\n",
        "                            if table_match:\n",
        "                                main_table = table_match.group(1)\n",
        "                                logger.debug(\n",
        "                                    f\"Main table for INSERT: {main_table}\")\n",
        "                        elif gen_type == \"UPDATE\":\n",
        "                            table_match = re.search(\n",
        "                                r'UPDATE\\s+([^\\s(]+)', gen_sql, re.IGNORECASE)\n",
        "                            if table_match:\n",
        "                                main_table = table_match.group(1)\n",
        "                                logger.debug(\n",
        "                                    f\"Main table for UPDATE: {main_table}\")\n",
        "                        elif gen_type == \"DELETE\":\n",
        "                            table_match = re.search(\n",
        "                                r'DELETE\\s+FROM\\s+([^\\s(]+)', gen_sql, re.IGNORECASE)\n",
        "                            if table_match:\n",
        "                                main_table = table_match.group(1)\n",
        "                                logger.debug(\n",
        "                                    f\"Main table for DELETE: {main_table}\")\n",
        "\n",
        "                        if 'main_table' in locals():\n",
        "                            exists = check_table_exists(conn, main_table)\n",
        "                            logger.debug(\n",
        "                                f\"Main table '{main_table}' exists: {exists}\")\n",
        "\n",
        "                    gen_sql_fixed = fix_case_sensitivity_in_sql(conn, gen_sql)\n",
        "\n",
        "                    if gen_sql_fixed != gen_sql:\n",
        "                        logger.debug(\n",
        "                            f\"Fixed case sensitivity issues in DML statement\")\n",
        "                        gen_sql = gen_sql_fixed\n",
        "\n",
        "                    converted_gen_sql = convert_sql_to_sqlite(gen_sql)\n",
        "                    logger.debug(\n",
        "                        f\"Executing DML statement: {converted_gen_sql[:200]}...\")\n",
        "\n",
        "                    cursor.execute(converted_gen_sql)\n",
        "                    reward = 0.5 * REWARD_WEIGHTS[\"sql_correctness\"]\n",
        "\n",
        "                except sqlite3.Error as e:\n",
        "                    error_msg = str(e)\n",
        "                    logger.warning(\n",
        "                        f\"Error executing DML statement: {error_msg}\")\n",
        "                    logger.warning(f\"Generated SQL: {gen_sql[:200]}...\")\n",
        "\n",
        "                    if \"no such table\" in error_msg.lower():\n",
        "                        table_match = re.search(\n",
        "                            r\"no such table: (\\w+)\", error_msg, re.IGNORECASE)\n",
        "                        if table_match:\n",
        "                            missing_table = table_match.group(1)\n",
        "                            logger.debug(f\"Missing table: {missing_table}\")\n",
        "\n",
        "                            all_tables = list_all_tables(conn)\n",
        "                            logger.debug(f\"Available tables: {all_tables}\")\n",
        "\n",
        "                            case_mapping = {t.lower(): t for t in all_tables}\n",
        "\n",
        "                            if missing_table.lower() in case_mapping:\n",
        "                                correct_case = case_mapping[missing_table.lower(\n",
        "                                )]\n",
        "                                logger.debug(\n",
        "                                    f\"Case mismatch detected! '{missing_table}' vs '{correct_case}'\")\n",
        "\n",
        "                                corrected_sql = re.sub(r'\\b' + re.escape(missing_table) + r'\\b',\n",
        "                                                       correct_case,\n",
        "                                                       gen_sql,\n",
        "                                                       flags=re.IGNORECASE)\n",
        "\n",
        "                                logger.debug(\n",
        "                                    f\"Corrected SQL: {corrected_sql[:200]}...\")\n",
        "\n",
        "                                try:\n",
        "                                    converted_corrected = convert_sql_to_sqlite(\n",
        "                                        corrected_sql)\n",
        "                                    cursor.execute(converted_corrected)\n",
        "                                    reward = 0.4 * \\\n",
        "                                        REWARD_WEIGHTS[\"sql_correctness\"]\n",
        "                                    logger.debug(\n",
        "                                        f\"Execution successful after case correction!\")\n",
        "                                except sqlite3.Error as e2:\n",
        "                                    logger.warning(\n",
        "                                        f\"Still failed after case correction: {e2}\")\n",
        "                                    logger.debug(\n",
        "                                        f\"New error after case correction: {e2}\")\n",
        "                                    logger.debug(\n",
        "                                        f\"Converted corrected SQL: {converted_corrected[:200]}...\")\n",
        "\n",
        "            rewards.append(reward)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in execution reward calculation: {e}\")\n",
        "            import traceback\n",
        "            logger.warning(f\"Stack trace: {traceback.format_exc()}\")\n",
        "            reward = 0.0\n",
        "            rewards.append(reward)\n",
        "        finally:\n",
        "            logger.debug(f\"Final reward for completion {i}: {reward}\")\n",
        "\n",
        "            if conn:\n",
        "                try:\n",
        "                    conn.close()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            try:\n",
        "                if temp_db_file and os.path.exists(temp_db_file):\n",
        "                    os.unlink(temp_db_file)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def sql_accuracy_from_exec_reward(prompts, completions, references, **kwargs) -> list[float]:\n",
        "    exec_rewards = execute_query_reward_func(prompts, completions, references, **kwargs)\n",
        "    full = float(REWARD_WEIGHTS[\"sql_correctness\"])  # 1.2\n",
        "    eps = 1e-9\n",
        "    return [1.0 if (r is not None and r >= full - eps) else 0.0 for r in exec_rewards]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    DEBUG_MODE = True\n",
        "    log_level = logging.DEBUG\n",
        "    logging.getLogger().setLevel(log_level)\n",
        "    for handler in logging.getLogger().handlers:\n",
        "        handler.setLevel(log_level)\n",
        "    logger.info(\"Running example usage...\")\n",
        "\n",
        "    prompts_example = [\"Show names of dogs older than 5 years.\"]\n",
        "    completions_example = [\n",
        "        \"<reasoning>Find dogs table. Filter by age > 5. Select name column.</reasoning>\\n<answer>SELECT name FROM dogs WHERE age > 5;</answer>\"\n",
        "    ]\n",
        "    references_example = [[{\n",
        "        \"gold_sql\": \"SELECT name FROM dogs WHERE age > 5 ORDER BY dog_id;\",\n",
        "        \"sql_context\": \"\"\"\n",
        "        CREATE TABLE dogs (dog_id INTEGER PRIMARY KEY, name TEXT, age INTEGER);\n",
        "        INSERT INTO dogs (name, age) VALUES ('Buddy', 7);\n",
        "        INSERT INTO dogs (name, age) VALUES ('Lucy', 4);\n",
        "        INSERT INTO dogs (name, age) VALUES ('Max', 8);\n",
        "        \"\"\",\n",
        "        \"question\": prompts_example[0]\n",
        "    }]]\n",
        "\n",
        "    print(\"\\n--- Testing execute_query_reward_func (Order Ignored) ---\")\n",
        "    exec_rewards_order_ignored = execute_query_reward_func(\n",
        "        prompts_example, completions_example, references_example,\n",
        "        source_dialect_dataset=\"mysql\",\n",
        "        source_dialect_generated=\"postgresql\",\n",
        "        order_matters=False,\n",
        "        validate_schema=True\n",
        "    )\n",
        "    print(f\"Execution Rewards (Order Ignored): {exec_rewards_order_ignored}\")\n",
        "\n",
        "    print(\"\\n--- Testing execute_query_reward_func (Order Matters) ---\")\n",
        "    exec_rewards_order_matters = execute_query_reward_func(\n",
        "        prompts_example, completions_example, references_example,\n",
        "        source_dialect_dataset=\"mysql\",\n",
        "        source_dialect_generated=\"postgresql\",\n",
        "        order_matters=True,\n",
        "        validate_schema=True\n",
        "    )\n",
        "    print(f\"Execution Rewards (Order Matters): {exec_rewards_order_matters}\")\n",
        "\n",
        "    print(\"\\n--- Testing Complexity Reward ---\")\n",
        "    complexity_rewards = complexity_reward(\n",
        "        prompts_example, completions_example, references_example)\n",
        "    print(f\"Complexity Rewards: {complexity_rewards}\")\n",
        "\n",
        "    print(\"\\n--- Testing Reasoning Quality Reward ---\")\n",
        "    reasoning_rewards = reasoning_quality_reward(\n",
        "        prompts_example, completions_example, references_example)\n",
        "    print(f\"Reasoning Quality Rewards: {reasoning_rewards}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad72f628",
      "metadata": {
        "id": "ad72f628"
      },
      "source": [
        "### 6.2 OpenR1-Math rewards\n",
        "\n",
        "Используем `math_verify` для корректности финального ответа, а также вспомогательные компоненты:\n",
        "- reasoning_steps_reward,\n",
        "- cosine-scaled reward,\n",
        "- repetition penalty.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b077681",
      "metadata": {
        "id": "1b077681"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2025 The HuggingFace Team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Reward functions for GRPO training.\"\"\"\n",
        "\n",
        "import asyncio\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "from functools import partial, update_wrapper\n",
        "from typing import Callable, Dict, Literal, Optional, List\n",
        "from statistics import mean\n",
        "from latex2sympy2_extended import NormalizationConfig\n",
        "from math_verify import LatexExtractionConfig, ExprExtractionConfig, parse as math_parse, verify as math_verify\n",
        "\n",
        "\n",
        "_LATEX_INLINE_RE  = re.compile(r\"^\\\\\\((.*)\\\\\\)$\", re.DOTALL)\n",
        "_LATEX_DISPLAY_RE = re.compile(r\"^\\\\\\[(.*)\\\\\\]$\", re.DOTALL)\n",
        "\n",
        "def normalize_answer_for_verifier(ans: str | None) -> str | None:\n",
        "    if ans is None:\n",
        "        return None\n",
        "    s = ans.strip()\n",
        "    if not s:\n",
        "        return None\n",
        "\n",
        "    # unwrap $...$\n",
        "    if len(s) >= 2 and s[0] == \"$\" and s[-1] == \"$\":\n",
        "        s = s[1:-1].strip()\n",
        "\n",
        "    # unwrap \\( ... \\)\n",
        "    m = _LATEX_INLINE_RE.match(s)\n",
        "    if m:\n",
        "        s = m.group(1).strip()\n",
        "\n",
        "    # unwrap \\[ ... \\]\n",
        "    m = _LATEX_DISPLAY_RE.match(s)\n",
        "    if m:\n",
        "        s = m.group(1).strip()\n",
        "\n",
        "    # unwrap \\boxed{...} (часто встречается)\n",
        "    m = re.match(r\"^\\\\boxed\\s*\\{(.*)\\}\\s*$\", s, flags=re.DOTALL)\n",
        "    if m:\n",
        "        s = m.group(1).strip()\n",
        "\n",
        "    # убрать частые лейблы в начале\n",
        "    s = re.sub(r\"^(?:answer|result|center|centre|radius)\\s*[:=]\\s*\", \"\", s, flags=re.IGNORECASE)\n",
        "\n",
        "    # схлопнуть переносы/пробелы\n",
        "    s = s.replace(\"\\u00A0\", \" \")\n",
        "    s = \" \".join(s.split())\n",
        "\n",
        "    # убрать хвостовую пунктуацию\n",
        "    s = s.rstrip(\" .;,\")\n",
        "\n",
        "    return s if s else None\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_reward(completions: list[list[dict[str, str]]], solution: list[str], **kwargs) -> list[Optional[float]]:\n",
        "    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n",
        "    contents = [_extract_answer(_comp_to_text(c)) for c in completions]\n",
        "    rewards = []\n",
        "    for content, sol in zip(contents, solution):\n",
        "        content = normalize_answer_for_verifier(content)\n",
        "        gold_parsed = math_parse(\n",
        "            sol,\n",
        "            extraction_config=[\n",
        "                    LatexExtractionConfig(\n",
        "                        normalization_config=NormalizationConfig(\n",
        "                            nits=True,\n",
        "                            malformed_operators=True,\n",
        "                            basic_latex=True,\n",
        "                            equations=True,\n",
        "                            boxed=\"all\",\n",
        "                            units=True,\n",
        "                        ),\n",
        "                        # Ensures that boxed is tried first\n",
        "                        boxed_match_priority=0,\n",
        "                        try_extract_without_anchor=True,\n",
        "                    ),\n",
        "                    ExprExtractionConfig(),\n",
        "                ],\n",
        "                extraction_mode=\"first_match\",\n",
        "\n",
        "        )\n",
        "        if len(gold_parsed) != 0:\n",
        "            # We require the answer to be provided in correct latex (no malformed operators)\n",
        "            answer_parsed = math_parse(\n",
        "                content,\n",
        "                extraction_config=[\n",
        "                    LatexExtractionConfig(\n",
        "                        normalization_config=NormalizationConfig(\n",
        "                            nits=True,\n",
        "                            malformed_operators=True,\n",
        "                            basic_latex=True,\n",
        "                            equations=True,\n",
        "                            boxed=\"all\",\n",
        "                            units=True,\n",
        "                        ),\n",
        "                        # Ensures that boxed is tried first\n",
        "                        boxed_match_priority=0,\n",
        "                        try_extract_without_anchor=False,\n",
        "                    ),\n",
        "                    ExprExtractionConfig(),\n",
        "                ],\n",
        "                extraction_mode=\"first_match\",\n",
        "            )\n",
        "            if len(answer_parsed) == 0:\n",
        "                answer_parsed = math_parse(\n",
        "                content,\n",
        "                extraction_config=[\n",
        "                    LatexExtractionConfig(\n",
        "                        normalization_config=NormalizationConfig(\n",
        "                            nits=True,\n",
        "                            malformed_operators=True,\n",
        "                            basic_latex=True,\n",
        "                            equations=True,\n",
        "                            boxed=\"all\",\n",
        "                            units=True,\n",
        "                        ),\n",
        "                        # Ensures that boxed is tried first\n",
        "                        boxed_match_priority=0,\n",
        "                        try_extract_without_anchor=True,\n",
        "                    ),\n",
        "                    ExprExtractionConfig(),\n",
        "                ],\n",
        "                extraction_mode=\"first_match\",\n",
        "            )\n",
        "            # Compute binary rewards if verifiable, `None` otherwise to skip this example\n",
        "            try:\n",
        "                reward = float(math_verify(gold_parsed, answer_parsed))\n",
        "            except Exception as e:\n",
        "                print(f\"verify failed: {e}, answer: {answer_parsed}, gold: {gold_parsed}\")\n",
        "                reward = None\n",
        "        else:\n",
        "            # If the gold solution is not parseable, we assign `None` to skip this example\n",
        "            reward = None\n",
        "            print(\"Failed to parse gold solution: \", sol)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "_REAS_RE = re.compile(r\"<reasoning>\\s*(.*?)\\s*</reasoning>\", re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "def _extract_reasoning(txt: str) -> str:\n",
        "    m = _REAS_RE.search(txt)\n",
        "    return m.group(1).strip() if m else \"\"\n",
        "\n",
        "def reasoning_steps_reward(completions, **kwargs):\n",
        "    r\"\"\"Reward function that checks for clear step-by-step reasoning.\n",
        "    Regex pattern:\n",
        "        Step \\d+: - matches \"Step 1:\", \"Step 2:\", etc.\n",
        "        ^\\d+\\. - matches numbered lists like \"1.\", \"2.\", etc. at start of line\n",
        "        \\n- - matches bullet points with hyphens\n",
        "        \\n\\* - matches bullet points with asterisks\n",
        "        First,|Second,|Next,|Finally, - matches transition words\n",
        "    \"\"\"\n",
        "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
        "    texts = [_extract_reasoning(_comp_to_text(c)) for c in completions]\n",
        "    matches = [len(re.findall(pattern, t)) for t in texts]\n",
        "    return [min(1.0, count / 3) for count in matches]\n",
        "\n",
        "\n",
        "def get_cosine_scaled_reward(\n",
        "    min_value_wrong: float = -1.0,\n",
        "    max_value_wrong: float = -0.5,\n",
        "    min_value_correct: float = 0.5,\n",
        "    max_value_correct: float = 1.0,\n",
        "    max_len: int = 2048,\n",
        "):\n",
        "    def cosine_scaled_reward(completions, solution, **kwargs):\n",
        "        \"\"\"Reward function that scales based on completion length using a cosine schedule.\n",
        "\n",
        "        Shorter correct solutions are rewarded more than longer ones.\n",
        "        Longer incorrect solutions are penalized less than shorter ones.\n",
        "\n",
        "        Args:\n",
        "            completions: List of model completions\n",
        "            solution: List of ground truth solutions\n",
        "\n",
        "        This function is parameterized by the following arguments:\n",
        "            min_value_wrong: Minimum reward for wrong answers\n",
        "            max_value_wrong: Maximum reward for wrong answers\n",
        "            min_value_correct: Minimum reward for correct answers\n",
        "            max_value_correct: Maximum reward for correct answers\n",
        "            max_len: Maximum length for scaling\n",
        "        \"\"\"\n",
        "        contents = [_extract_answer(_comp_to_text(c)) for c in completions]\n",
        "        rewards = []\n",
        "\n",
        "        # Глобальный теоретический минимум/максимум по параметрам\n",
        "        min_possible = min(min_value_wrong, max_value_wrong, min_value_correct, max_value_correct)\n",
        "        max_possible = max(min_value_wrong, max_value_wrong, min_value_correct, max_value_correct)\n",
        "\n",
        "        for content, sol in zip(contents, solution):\n",
        "            content = normalize_answer_for_verifier(content)\n",
        "            if content is None:\n",
        "                rewards.append(0.0)\n",
        "                continue\n",
        "            gold_parsed = math_parse(\n",
        "                sol,\n",
        "                extraction_config=[\n",
        "                    LatexExtractionConfig(\n",
        "                        normalization_config=NormalizationConfig(\n",
        "                            nits=True,\n",
        "                            malformed_operators=True,\n",
        "                            basic_latex=True,\n",
        "                            equations=True,\n",
        "                            boxed=\"all\",\n",
        "                            units=True,\n",
        "                        ),\n",
        "                        # Ensures that boxed is tried first\n",
        "                        boxed_match_priority=0,\n",
        "                        try_extract_without_anchor=True,\n",
        "                    ),\n",
        "                    ExprExtractionConfig(),\n",
        "                ],\n",
        "                extraction_mode=\"first_match\",\n",
        "            )\n",
        "            if len(gold_parsed) == 0:\n",
        "                # Skip unparseable examples — даём максимальный raw_reward\n",
        "                raw_reward = max_value_correct\n",
        "                print(\"Failed to parse gold solution: \", sol)\n",
        "            else:\n",
        "                answer_parsed = math_parse(\n",
        "                    content,\n",
        "                    extraction_config=[\n",
        "                        LatexExtractionConfig(\n",
        "                            normalization_config=NormalizationConfig(\n",
        "                                nits=True,\n",
        "                                malformed_operators=True,\n",
        "                                basic_latex=True,\n",
        "                                equations=True,\n",
        "                                boxed=True,\n",
        "                                units=True,\n",
        "                            ),\n",
        "                            boxed_match_priority=0,\n",
        "                            try_extract_without_anchor=False,\n",
        "                        ),\n",
        "                        ExprExtractionConfig(),\n",
        "                    ],\n",
        "                    extraction_mode=\"first_match\",\n",
        "                )\n",
        "                if len(answer_parsed) == 0:\n",
        "                    answer_parsed = math_parse(\n",
        "                    content,\n",
        "                    extraction_config=[\n",
        "                        LatexExtractionConfig(\n",
        "                            normalization_config=NormalizationConfig(\n",
        "                                nits=True,\n",
        "                                malformed_operators=True,\n",
        "                                basic_latex=True,\n",
        "                                equations=True,\n",
        "                                boxed=\"all\",\n",
        "                                units=True,\n",
        "                            ),\n",
        "                            # Ensures that boxed is tried first\n",
        "                            boxed_match_priority=0,\n",
        "                            try_extract_without_anchor=True,\n",
        "                        ),\n",
        "                        ExprExtractionConfig(),\n",
        "                    ],\n",
        "                    extraction_mode=\"first_match\",\n",
        "                )\n",
        "                is_correct = math_verify(gold_parsed, answer_parsed)\n",
        "                if content is None:\n",
        "                    rewards.append(0.0)\n",
        "                    continue\n",
        "                gen_len = len(content)\n",
        "\n",
        "                # Apply cosine scaling based on length\n",
        "                progress = gen_len / max_len\n",
        "                cosine = math.cos(progress * math.pi)\n",
        "\n",
        "                if is_correct:\n",
        "                    min_value = min_value_correct\n",
        "                    max_value = max_value_correct\n",
        "                else:\n",
        "                    # Swap min/max for incorrect answers\n",
        "                    min_value = max_value_wrong\n",
        "                    max_value = min_value_wrong\n",
        "\n",
        "                raw_reward = min_value + 0.5 * (max_value - min_value) * (1.0 + cosine)\n",
        "\n",
        "            # Нормализация: raw_reward из [min_possible, max_possible] → [0, 1]\n",
        "            if max_possible == min_possible:\n",
        "                norm_reward = 0.5\n",
        "            else:\n",
        "                norm_reward = (raw_reward - min_possible) / (max_possible - min_possible)\n",
        "\n",
        "            # жёсткий клип на всякий случай\n",
        "            norm_reward = max(0.0, min(1.0, norm_reward))\n",
        "            rewards.append(float(norm_reward))\n",
        "\n",
        "        return rewards\n",
        "\n",
        "    return cosine_scaled_reward\n",
        "\n",
        "\n",
        "def get_repetition_penalty_reward(ngram_size: int = 4, max_penalty: float = -0.5, language: str = \"en\"):\n",
        "    \"\"\"\n",
        "    Computes N-gram repetition penalty as described in Appendix C.2 of https://huggingface.co/papers/2502.03373.\n",
        "    Reference implementation from: https://github.com/eddycmu/demystify-long-cot/blob/release/openrlhf/openrlhf/reward/repetition.py\n",
        "\n",
        "    Args:\n",
        "    ngram_size: size of the n-grams\n",
        "    max_penalty: Maximum (negative) penalty for wrong answers\n",
        "    language: Language of the text, defaults to `en`. Used to choose the way to split the text into n-grams.\n",
        "    \"\"\"\n",
        "    if max_penalty > 0:\n",
        "        raise ValueError(f\"max_penalty {max_penalty} should not be positive\")\n",
        "\n",
        "    if language == \"en\":\n",
        "\n",
        "        def zipngram(text: str, ngram_size: int):\n",
        "            words = text.lower().split()\n",
        "            return zip(*[words[i:] for i in range(ngram_size)]), words\n",
        "\n",
        "    elif language == \"zh\":\n",
        "        from transformers.utils.import_utils import _is_package_available\n",
        "\n",
        "        if not _is_package_available(\"jieba\"):\n",
        "            raise ValueError(\"Please install jieba to use Chinese language\")\n",
        "\n",
        "        def zipngram(text: str, ngram_size: int):\n",
        "            import jieba\n",
        "\n",
        "            seg_list = list(jieba.cut(text))\n",
        "            return zip(*[seg_list[i:] for i in range(ngram_size)]), seg_list\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Word splitting for language `{language}` is not yet implemented. Please implement your own zip-ngram function.\"\n",
        "        )\n",
        "\n",
        "    def repetition_penalty_reward(completions, **kwargs) -> list[float]:\n",
        "        \"\"\"\n",
        "        reward function the penalizes repetitions\n",
        "        ref implementation: https://github.com/eddycmu/demystify-long-cot/blob/release/openrlhf/openrlhf/reward/repetition.py\n",
        "\n",
        "        Args:\n",
        "            completions: List of model completions\n",
        "        \"\"\"\n",
        "\n",
        "        contents = [_extract_answer(_comp_to_text(c)) for c in completions]\n",
        "        rewards = []\n",
        "        for completion in contents:\n",
        "            if completion == \"\":\n",
        "                rewards.append(0.0)\n",
        "                continue\n",
        "\n",
        "            ngrams = set()\n",
        "            total = 0\n",
        "            ngram_array, words = zipngram(completion, ngram_size)\n",
        "\n",
        "            if len(words) < ngram_size:\n",
        "                rewards.append(0.0)\n",
        "                continue\n",
        "\n",
        "            for ng in ngram_array:\n",
        "                ngrams.add(ng)\n",
        "                total += 1\n",
        "\n",
        "            scaling = 1 - len(ngrams) / total\n",
        "            reward = scaling * max_penalty\n",
        "            rewards.append(reward)\n",
        "        return rewards\n",
        "\n",
        "    return repetition_penalty_reward\n",
        "\n",
        "def summarize_rewards(name, values):\n",
        "    \"\"\"Печать простых статистик по вектору ревардов.\"\"\"\n",
        "    vals = [v for v in values if v is not None]\n",
        "    if not vals:\n",
        "        print(f\"{name}: no values\")\n",
        "        return\n",
        "    print(\n",
        "        f\"{name}: n={len(vals)}, \"\n",
        "        f\"mean={mean(vals):.4f}, \"\n",
        "        f\"min={min(vals):.4f}, \"\n",
        "        f\"max={max(vals):.4f}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc3e02e5",
      "metadata": {
        "id": "fc3e02e5"
      },
      "source": [
        "### 6.3 Coding (Python) rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "124aa28b",
      "metadata": {
        "id": "124aa28b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import tempfile\n",
        "from typing import Dict, Tuple\n",
        "from datasets import load_dataset\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "def _ref_to_dict(ref) -> dict:\n",
        "    \"\"\"Normalize references item to dict. Supports JSON-string (new) and legacy dict/list.\"\"\"\n",
        "    try:\n",
        "        if ref is None:\n",
        "            return {}\n",
        "        if isinstance(ref, str):\n",
        "            s = ref.strip()\n",
        "            if not s:\n",
        "                return {}\n",
        "            try:\n",
        "                v = json.loads(s)\n",
        "                return v if isinstance(v, dict) else {}\n",
        "            except Exception:\n",
        "                return {}\n",
        "        if isinstance(ref, dict):\n",
        "            return ref\n",
        "        if isinstance(ref, list) and ref:\n",
        "            x = ref[0]\n",
        "            if isinstance(x, dict):\n",
        "                return x\n",
        "            if isinstance(x, str):\n",
        "                try:\n",
        "                    v = json.loads(x)\n",
        "                    return v if isinstance(v, dict) else {}\n",
        "                except Exception:\n",
        "                    return {}\n",
        "        return {}\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "# -----------------------------\n",
        "# Shared helpers\n",
        "# -----------------------------\n",
        "_ANSWER_EXTRACT_RE = re.compile(r\"<answer>\\s*(.*?)\\s*</answer>\", re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "def _get_answer_text(completion_text: str) -> str:\n",
        "    \"\"\"Extract inner <answer> ... </answer> if present, else return whole text.\"\"\"\n",
        "    if not completion_text:\n",
        "        return \"\"\n",
        "    m = _ANSWER_EXTRACT_RE.search(completion_text)\n",
        "    return (m.group(1).strip() if m else completion_text.strip())\n",
        "\n",
        "def extract_fenced_code(text: str, language: str) -> str:\n",
        "    \"\"\"Return last fenced code block ```{language} ...``` from text (case-insensitive).\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    pattern = re.compile(rf\"```{re.escape(language)}\\s*\\n(.*?)```\", re.DOTALL | re.IGNORECASE)\n",
        "    matches = pattern.findall(text)\n",
        "    return (matches[-1] if matches else \"\").strip()\n",
        "\n",
        "def _completion_content(completion) -> str:\n",
        "    \"\"\"Normalize TRL completion format (list[{'content':...}] / dict / str) to plain text.\"\"\"\n",
        "    try:\n",
        "        if isinstance(completion, list) and completion:\n",
        "            return str(completion[-1].get(\"content\", \"\"))\n",
        "        if isinstance(completion, dict):\n",
        "            return str(completion.get(\"content\", \"\"))\n",
        "        return str(completion)\n",
        "    except Exception:\n",
        "        return str(completion)\n",
        "\n",
        "def _extract_python_code(completion_text: str) -> str:\n",
        "    \"\"\"Extract Python code from completion.\"\"\"\n",
        "    text = completion_text.replace(\"<end_of_turn>\", \"\")\n",
        "    ans = _get_answer_text(text)\n",
        "    code = extract_fenced_code(ans, \"python\")\n",
        "    if not code:\n",
        "        # Fallback: try to use answer content directly\n",
        "        code = ans.strip()\n",
        "    return code\n",
        "\n",
        "\n",
        "def code_execution_reward_func(prompts=None, completions=None, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Reward for Python code that executes without errors.\n",
        "\n",
        "    Returns:\n",
        "        list[float]: 0.5 if code runs successfully, -0.25 otherwise.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for c in completions:\n",
        "        text = _completion_content(c)\n",
        "        code = _extract_python_code(text)\n",
        "\n",
        "        if not code:\n",
        "            out.append(-0.25)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with tempfile.TemporaryDirectory() as td:\n",
        "                path = os.path.join(td, \"main.py\")\n",
        "                with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(code + \"\\n\")\n",
        "\n",
        "                proc = subprocess.run(\n",
        "                    [\"python3\", path],\n",
        "                    input=\"\",\n",
        "                    text=True,\n",
        "                    capture_output=True,\n",
        "                    timeout=5.0,\n",
        "                )\n",
        "\n",
        "                out.append(0.5 if proc.returncode == 0 else -0.25)\n",
        "        except Exception:\n",
        "            out.append(-0.25)\n",
        "\n",
        "    return out\n",
        "\n",
        "def _normalize_output(s: str) -> str:\n",
        "    \"\"\"Whitespace-insensitive compare (robust to trailing spaces/newlines).\"\"\"\n",
        "    return \" \".join(str(s).strip().split())\n",
        "\n",
        "\n",
        "def code_reward_func(prompts=None, completions=None, references=None, timeout_s: float = 5.0, **kwargs):\n",
        "    \"\"\"\n",
        "    Execute Python solutions on verification_info.test_cases.\n",
        "\n",
        "    Expected: references[i] contains {\"verification_info\": {\"test_cases\": [{\"input\":..., \"output\":...}, ...]}}\n",
        "    Reward: pass_rate in [0, 1].\n",
        "    \"\"\"\n",
        "    import math\n",
        "\n",
        "    out = []\n",
        "    references = references or [{} for _ in range(len(completions))]\n",
        "\n",
        "    for c, ref in zip(completions, references):\n",
        "        ref = _ref_to_dict(ref)\n",
        "        text = _completion_content(c)\n",
        "        code = _extract_python_code(text)\n",
        "\n",
        "        if not code:\n",
        "            out.append(0.0)\n",
        "            continue\n",
        "\n",
        "        ref = _ref_to_dict(ref)\n",
        "        vinfo = ref.get(\"verification_info\") or {}\n",
        "        test_cases = vinfo.get(\"test_cases\") or []\n",
        "\n",
        "        if not isinstance(test_cases, list) or len(test_cases) == 0:\n",
        "            out.append(0.0)\n",
        "            continue\n",
        "\n",
        "        passed = 0\n",
        "        total = len(test_cases)\n",
        "\n",
        "        with tempfile.TemporaryDirectory() as td:\n",
        "            path = os.path.join(td, \"main.py\")\n",
        "            with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(code + \"\\n\")\n",
        "\n",
        "            for tc in test_cases:\n",
        "                inp = tc.get(\"input\", \"\")\n",
        "                exp = tc.get(\"output\", \"\")\n",
        "                try:\n",
        "                    proc = subprocess.run(\n",
        "                        [\"python3\", path],\n",
        "                        input=inp,\n",
        "                        text=True,\n",
        "                        capture_output=True,\n",
        "                        timeout=float(timeout_s),\n",
        "                    )\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "                if proc.returncode != 0:\n",
        "                    continue\n",
        "\n",
        "                if _normalize_output(proc.stdout) == _normalize_output(exp):\n",
        "                    passed += 1\n",
        "\n",
        "        # Return pass rate (accuracy) - can be modified to accuracy^3 * 2 if needed\n",
        "        out.append(passed / max(1, total))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def soft_code_format_reward_func(prompts=None, completions=None, language: str = \"python\", **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Soft reward for code format with configurable language.\n",
        "\n",
        "    Args:\n",
        "        prompts: Not used, for compatibility\n",
        "        completions: List of model completions\n",
        "        language: Programming language to check for (\"python\", \"cpp\", \"c\", etc.)\n",
        "\n",
        "    Returns graduated reward:\n",
        "        - 0.25: Full format (<reasoning>...</reasoning><answer>```{language}...```</answer>)\n",
        "        - 0.15: Has answer with code block in specified language\n",
        "        - 0.10: Has answer tags\n",
        "        - 0.05: Has code block in specified language anywhere\n",
        "        - 0.0: No recognizable format\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for c in completions:\n",
        "        text = _completion_content(c).replace(\"<end_of_turn>\", \"\")\n",
        "        reward = 0.0\n",
        "\n",
        "        has_reasoning = bool(re.search(r\"<reasoning>.*?</reasoning>\", text, re.S))\n",
        "        has_answer = bool(re.search(r\"<answer>.*?</answer>\", text, re.S))\n",
        "        answer_content = _get_answer_text(text)\n",
        "        has_code_in_answer = bool(extract_fenced_code(answer_content, language))\n",
        "        has_code_anywhere = bool(extract_fenced_code(text, language))\n",
        "\n",
        "        if has_reasoning and has_answer and has_code_in_answer:\n",
        "            reward = 0.25\n",
        "        elif has_answer and has_code_in_answer:\n",
        "            reward = 0.15\n",
        "        elif has_answer:\n",
        "            reward = 0.10\n",
        "        elif has_code_anywhere:\n",
        "            reward = 0.05\n",
        "\n",
        "        out.append(reward)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# Convenience wrappers for specific languages\n",
        "def soft_python_format_reward_func(prompts=None, completions=None, **kwargs) -> list[float]:\n",
        "    \"\"\"Soft format reward for Python code.\"\"\"\n",
        "    return soft_code_format_reward_func(prompts, completions, language=\"python\", **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f5a3718",
      "metadata": {
        "id": "9f5a3718"
      },
      "source": [
        "### 6.5 Агрегация reward-компонент и логирование в TensorBoard\n",
        "\n",
        "Для mixed batches мы считаем нужные компоненты **только для соответствующего task** и агрегируем их взвешенно.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a1f290",
      "metadata": {
        "id": "78a1f290"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "# -----------------------------\n",
        "# Strict format reward (all tasks)\n",
        "# -----------------------------\n",
        "_FMT_RE_TAGS = re.compile(\n",
        "    r\"<reasoning>\\s*(.*?)\\s*</reasoning>\\s*<answer>\\s*(.*?)\\s*</answer>\",\n",
        "    flags=re.DOTALL | re.IGNORECASE,\n",
        ")\n",
        "\n",
        "_FMT_RE_CODE_PY = re.compile(\n",
        "    r\"^\\s*```python\\s*\\r?\\n[\\s\\S]*?\\r?\\n```\\s*$\",\n",
        "    flags=re.IGNORECASE,\n",
        ")\n",
        "_FMT_RE_CODE_CPP = re.compile(\n",
        "    r\"^\\s*```cpp\\s*\\r?\\n[\\s\\S]*?\\r?\\n```\\s*$\",\n",
        "    flags=re.IGNORECASE,\n",
        ")\n",
        "\n",
        "def unified_format_reward(prompts, completions, task=None, **kwargs):\n",
        "    \"\"\"Task-aware strict format reward.\"\"\"\n",
        "    scores = []\n",
        "    for i, c in enumerate(completions):\n",
        "        t = completion_to_text(c).replace(\"<end_of_turn>\", \"\").strip()\n",
        "        m = _FMT_RE_TAGS.search(t)\n",
        "        if not m:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # strict: nothing outside tags\n",
        "        if _FMT_RE_TAGS.sub(\"\", t).strip():\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        ans_inner = (m.group(2) or \"\").strip()\n",
        "\n",
        "        cur_task = None\n",
        "        if task is not None:\n",
        "            try:\n",
        "                cur_task = task[i]\n",
        "            except Exception:\n",
        "                cur_task = None\n",
        "\n",
        "        if cur_task == \"code\":\n",
        "            scores.append(1.0 if _FMT_RE_CODE_PY.match(ans_inner) else 0.0)\n",
        "        elif cur_task == \"ioi\":\n",
        "            scores.append(1.0 if _FMT_RE_CODE_CPP.match(ans_inner) else 0.0)\n",
        "        else:\n",
        "            scores.append(1.0)\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def _normalize_completion(c):\n",
        "    if isinstance(c, str):\n",
        "        return [{\"content\": c}]\n",
        "    if isinstance(c, dict):\n",
        "        return [c]\n",
        "    if isinstance(c, list) and c and isinstance(c[0], dict) and \"content\" in c[0]:\n",
        "        return c\n",
        "    return [{\"content\": str(c)}]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Masking wrappers for mixed batches\n",
        "# -----------------------------\n",
        "def mask_reward_by_task(\n",
        "    reward_func: Callable,\n",
        "    task_name: str,\n",
        "    *,\n",
        "    needs_references: bool = False,\n",
        "    needs_solution: bool = False,\n",
        "):\n",
        "    def wrapped(prompts, completions, task, references=None, solution=None, **kwargs):\n",
        "        idx = [i for i, t in enumerate(task) if t == task_name]\n",
        "        out = [0.0] * len(task)\n",
        "        if not idx:\n",
        "            return out\n",
        "\n",
        "        prompts_s = [prompts[i] for i in idx]\n",
        "        completions_s = [_normalize_completion(completions[i]) for i in idx]\n",
        "\n",
        "        call_kwargs = {}\n",
        "        if needs_references:\n",
        "            call_kwargs[\"references\"] = [references[i] for i in idx] if references is not None else None\n",
        "        if needs_solution:\n",
        "            call_kwargs[\"solution\"] = [solution[i] for i in idx] if solution is not None else None\n",
        "\n",
        "        try:\n",
        "            rewards_s = reward_func(prompts=prompts_s, completions=completions_s, **call_kwargs, **kwargs)\n",
        "        except TypeError:\n",
        "            rewards_s = reward_func(completions=completions_s, **call_kwargs, **kwargs)\n",
        "\n",
        "        for j, r in zip(idx, rewards_s):\n",
        "            out[j] = 0.0 if r is None else float(r)\n",
        "        return out\n",
        "\n",
        "    return wrapped\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Masked components per task\n",
        "# -----------------------------\n",
        "sql_accuracy_masked = mask_reward_by_task(sql_accuracy_from_exec_reward, \"sql\", needs_references=True)\n",
        "sql_exec_reward_masked = mask_reward_by_task(execute_query_reward_func, \"sql\", needs_references=True)\n",
        "sql_complexity_reward_masked = mask_reward_by_task(complexity_reward, \"sql\", needs_references=True)\n",
        "sql_reasoning_reward_masked = mask_reward_by_task(reasoning_quality_reward, \"sql\", needs_references=True)\n",
        "\n",
        "r1_accuracy_reward_masked = mask_reward_by_task(accuracy_reward, \"math\", needs_solution=True)\n",
        "r1_reasoning_steps_reward_masked = mask_reward_by_task(reasoning_steps_reward, \"math\")\n",
        "_cosine_reward_train_fn = get_cosine_scaled_reward(max_len=2048)\n",
        "r1_cosine_reward_masked = mask_reward_by_task(_cosine_reward_train_fn, \"math\", needs_solution=True)\n",
        "_rep_reward_train_fn = get_repetition_penalty_reward()\n",
        "r1_repetition_penalty_reward_masked = mask_reward_by_task(_rep_reward_train_fn, \"math\")\n",
        "\n",
        "sudoku_correctness_reward_masked = mask_reward_by_task(sudoku_correctness_reward_func, \"sudoku\", needs_solution=True)\n",
        "sudoku_int_reward_masked = mask_reward_by_task(sudoku_int_reward_func, \"sudoku\")\n",
        "sudoku_grid_format_reward_masked = mask_reward_by_task(sudoku_grid_format_reward_func, \"sudoku\")\n",
        "\n",
        "soft_code_format_reward_masked = mask_reward_by_task(soft_python_format_reward_func, \"code\")\n",
        "code_reward_masked = mask_reward_by_task(code_reward_func, \"code\", needs_references=True)\n",
        "code_execution_reward_masked = mask_reward_by_task(code_execution_reward_func, \"code\")\n",
        "code_reasoning_steps_reward_masked = mask_reward_by_task(reasoning_steps_reward, \"code\")\n",
        "\n",
        "soft_ioi_format_reward_masked = mask_reward_by_task(soft_ioi_format_reward_func, \"ioi\")\n",
        "ioi_code_reward_masked = mask_reward_by_task(ioi_code_reward_func, \"ioi\", needs_references=True)\n",
        "ioi_reasoning_steps_reward_masked = mask_reward_by_task(reasoning_steps_reward, \"ioi\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e33ad1",
      "metadata": {
        "id": "53e33ad1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "TASK_COMPONENTS = {\n",
        "    \"sql\": [\"format\", \"sql_accuracy\", \"sql_exec\", \"sql_complexity\", \"sql_reasoning\"],\n",
        "    \"math\": [\"format\", \"math_accuracy\", \"math_reasoning_steps\", \"math_cosine\", \"math_repetition\"],\n",
        "    \"sudoku\": [\"format\", \"sudoku_correctness\", \"sudoku_int\", \"sudoku_grid_format\"],\n",
        "    \"code\": [\"format\", \"code\", \"soft_code_format\", \"code_reasoning_steps\"],\n",
        "    \"ioi\": [\"format\", \"ioi_code\", \"soft_ioi_format\", \"ioi_reasoning_steps\"],\n",
        "}\n",
        "\n",
        "TASK_WEIGHTS = {\n",
        "    \"sql\": {\n",
        "        \"sql_accuracy\": 1.0,\n",
        "        \"sql_exec\": 1.0,\n",
        "        \"format\": 0.15,\n",
        "        \"sql_complexity\": 0.10,\n",
        "        \"sql_reasoning\": 0.10,\n",
        "    },\n",
        "    \"math\": {\n",
        "        \"math_accuracy\": 1.0,\n",
        "        \"format\": 0.15,\n",
        "        \"math_reasoning_steps\": 0.20,\n",
        "        \"math_cosine\": 0.10,\n",
        "        \"math_repetition\": 0.10,\n",
        "    },\n",
        "    \"sudoku\": {\n",
        "        \"sudoku_correctness\": 2.0,\n",
        "        \"format\": 0.10,\n",
        "        \"sudoku_int\": 0.10,\n",
        "        \"sudoku_grid_format\": 0.10,\n",
        "    },\n",
        "    \"code\": {\n",
        "        \"code\": 1.0,\n",
        "        \"format\": 0.10,\n",
        "        \"soft_code_format\": 0.15,\n",
        "        \"code_reasoning_steps\": 0.20,\n",
        "    },\n",
        "    \"ioi\": {\n",
        "        \"ioi_code\": 1.0,\n",
        "        \"soft_ioi_format\": 0.15,\n",
        "        \"format\": 0.10,\n",
        "        \"ioi_reasoning_steps\": 0.20,\n",
        "    },\n",
        "}\n",
        "\n",
        "REWARD_CACHE = None\n",
        "GEN_LOGGER = None\n",
        "\n",
        "\n",
        "class RewardCache:\n",
        "    def __init__(self, task_name: str):\n",
        "        self.task_name = task_name\n",
        "        self.last_comp = None\n",
        "        self.last_final = None\n",
        "        self.last_task = None\n",
        "\n",
        "    def compute(self, prompts, completions, task, references=None, solution=None, **kwargs):\n",
        "        comp = {}\n",
        "\n",
        "        needed = set()\n",
        "        for t in set(task):\n",
        "            needed.update(TASK_COMPONENTS.get(t, [\"format\"]))\n",
        "        needed.add(\"format\")\n",
        "\n",
        "        if \"format\" in needed:\n",
        "            comp[\"format\"] = unified_format_reward(prompts, completions, task=task)\n",
        "\n",
        "        # SQL\n",
        "        if \"sql_accuracy\" in needed:\n",
        "            comp[\"sql_accuracy\"] = sql_accuracy_masked(prompts, completions, task, references=references)\n",
        "        if \"sql_exec\" in needed:\n",
        "            comp[\"sql_exec\"] = sql_exec_reward_masked(prompts, completions, task, references=references)\n",
        "        if \"sql_complexity\" in needed:\n",
        "            comp[\"sql_complexity\"] = sql_complexity_reward_masked(prompts, completions, task, references=references)\n",
        "        if \"sql_reasoning\" in needed:\n",
        "            comp[\"sql_reasoning\"] = sql_reasoning_reward_masked(prompts, completions, task, references=references)\n",
        "\n",
        "        # MATH\n",
        "        if \"math_accuracy\" in needed:\n",
        "            comp[\"math_accuracy\"] = r1_accuracy_reward_masked(prompts, completions, task, solution=solution)\n",
        "        if \"math_reasoning_steps\" in needed:\n",
        "            comp[\"math_reasoning_steps\"] = r1_reasoning_steps_reward_masked(prompts, completions, task)\n",
        "        if \"math_cosine\" in needed:\n",
        "            comp[\"math_cosine\"] = r1_cosine_reward_masked(prompts, completions, task, solution=solution)\n",
        "        if \"math_repetition\" in needed:\n",
        "            comp[\"math_repetition\"] = r1_repetition_penalty_reward_masked(prompts, completions, task)\n",
        "\n",
        "        # CODE\n",
        "        if \"code\" in needed:\n",
        "            comp[\"code\"] = code_reward_masked(prompts, completions, task, references=references)\n",
        "        if \"soft_code_format\" in needed:\n",
        "            comp[\"soft_code_format\"] = soft_code_format_reward_masked(prompts, completions, task)\n",
        "        if \"code_reasoning_steps\" in needed:\n",
        "            comp[\"code_reasoning_steps\"] = code_reasoning_steps_reward_masked(prompts, completions, task)\n",
        "\n",
        "        out = [0.0] * len(task)\n",
        "        for i, t in enumerate(task):\n",
        "            names = TASK_COMPONENTS.get(t, [\"format\"])\n",
        "            wmap = TASK_WEIGHTS.get(t, {})\n",
        "            num, den = 0.0, 0.0\n",
        "            for name in names:\n",
        "                w = float(wmap.get(name, 1.0))\n",
        "                v = float(comp[name][i])\n",
        "                num += w * v\n",
        "                den += w\n",
        "            out[i] = num / max(1e-8, den)\n",
        "\n",
        "        self.last_comp = comp\n",
        "        self.last_final = out\n",
        "        self.last_task = task\n",
        "        return out\n",
        "\n",
        "\n",
        "def _expand_to_n(x, n: int):\n",
        "    if x is None:\n",
        "        return None\n",
        "    if isinstance(x, str):\n",
        "        return [x] * n\n",
        "    if not isinstance(x, list):\n",
        "        x = list(x)\n",
        "\n",
        "    m = len(x)\n",
        "    if m == n:\n",
        "        return x\n",
        "    if m == 0:\n",
        "        return [None] * n\n",
        "\n",
        "    if n % m == 0:\n",
        "        g = n // m\n",
        "        return [x[i // g] for i in range(n)]\n",
        "    return [x[i % m] for i in range(n)]\n",
        "\n",
        "\n",
        "def make_reward_logged(default_task: str):\n",
        "    def _reward(prompts, completions, completion_ids=None, **kwargs):\n",
        "        n = len(completions)\n",
        "\n",
        "        prompts_ = prompts if (isinstance(prompts, list) and len(prompts) == n) else _expand_to_n(prompts, n)\n",
        "\n",
        "        task = kwargs.get(\"task\")\n",
        "        if task is None:\n",
        "            task_ = [default_task] * n\n",
        "        elif isinstance(task, str):\n",
        "            task_ = [task] * n\n",
        "        else:\n",
        "            task_ = _expand_to_n(task, n)\n",
        "\n",
        "        references = _expand_to_n(kwargs.get(\"references\"), n)\n",
        "        solution = _expand_to_n(kwargs.get(\"solution\"), n)\n",
        "\n",
        "        final = REWARD_CACHE.compute(\n",
        "            prompts=prompts_,\n",
        "            completions=completions,\n",
        "            task=task_,\n",
        "            references=references,\n",
        "            solution=solution,\n",
        "        )\n",
        "\n",
        "        if len(final) != n:\n",
        "            raise RuntimeError(f\"final rewards {len(final)} != completions {n}\")\n",
        "\n",
        "        if GEN_LOGGER is not None:\n",
        "            try:\n",
        "                comps_txt = [completion_to_text(c) for c in completions]\n",
        "                GEN_LOGGER.log_batch(\n",
        "                    task=default_task,\n",
        "                    prompts=prompts_,\n",
        "                    completions_text=comps_txt,\n",
        "                    solutions=solution,\n",
        "                    rewards=final,\n",
        "                    reward_components=REWARD_CACHE.last_comp,\n",
        "                )\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return final\n",
        "\n",
        "    return _reward\n",
        "\n",
        "\n",
        "class RewardTBCallback(TrainerCallback):\n",
        "    def __init__(self, writer: SummaryWriter, task_name: str, tag_prefix: str = \"train/rewards\"):\n",
        "        self.w = writer\n",
        "        self.task_name = task_name\n",
        "        self.p = f\"{tag_prefix}/{task_name}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _as_np(x):\n",
        "        try:\n",
        "            return np.array(x, dtype=float)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        global REWARD_CACHE\n",
        "        if REWARD_CACHE is None or REWARD_CACHE.last_comp is None or REWARD_CACHE.last_final is None:\n",
        "            return\n",
        "\n",
        "        step = int(state.global_step)\n",
        "\n",
        "        r = self._as_np(REWARD_CACHE.last_final)\n",
        "        if r is not None and r.size:\n",
        "            self.w.add_scalar(f\"{self.p}/final_mean\", float(r.mean()), step)\n",
        "            self.w.add_scalar(f\"{self.p}/final_std\", float(r.std()), step)\n",
        "\n",
        "        comp = REWARD_CACHE.last_comp\n",
        "        for name, vals in comp.items():\n",
        "            v = self._as_np(vals)\n",
        "            if v is None or not v.size:\n",
        "                continue\n",
        "\n",
        "            self.w.add_scalar(f\"{self.p}/{name}_mean\", float(v.mean()), step)\n",
        "            self.w.add_scalar(f\"{self.p}/{name}_std\", float(v.std()), step)\n",
        "\n",
        "            if name in (\"sql_exec\", \"sql_accuracy\", \"math_accuracy\", \"sudoku_correctness\", \"code\", \"ioi_code\"):\n",
        "                self.w.add_scalar(f\"{self.p}/{name}_pos_rate\", float((v > 0).mean()), step)\n",
        "\n",
        "        self.w.flush()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9d1d16",
      "metadata": {
        "id": "ff9d1d16"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Generation logger\n",
        "# ---------------------------\n",
        "# Пишем в один файл:\n",
        "# - prompt (или его хэш/срез),\n",
        "# - completion,\n",
        "# - solution (gold),\n",
        "# - reward и reward_components.\n",
        "#\n",
        "# Это нужно для \"post-mortem\" анализа: частые ошибки формата, провалы в SQL/Math,\n",
        "# краши выполнения кода и т.п.\n",
        "\n",
        "import atexit\n",
        "import gzip\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from threading import Lock\n",
        "\n",
        "\n",
        "class SingleFileGenerationLogger:\n",
        "    \"\"\"Пишет генерации/реварды в один jsonl(.gz) файл (удобно для анализа ошибок).\"\"\"\n",
        "\n",
        "    def __init__(self, filepath: str, compress: bool = True, flush_every_lines: int = 200):\n",
        "        self.path = Path(filepath)\n",
        "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        self.compress = compress\n",
        "        self.flush_every = int(flush_every_lines)\n",
        "        self.lock = Lock()\n",
        "        self.trainer = None\n",
        "        self._buf = []\n",
        "        self._lines = 0\n",
        "        self._fh = (\n",
        "            gzip.open(self.path, \"at\", encoding=\"utf-8\")\n",
        "            if compress\n",
        "            else open(self.path, \"a\", encoding=\"utf-8\")\n",
        "        )\n",
        "        atexit.register(self.close)\n",
        "\n",
        "    def attach_trainer(self, trainer):\n",
        "        self.trainer = trainer\n",
        "\n",
        "    def log_batch(self, task, prompts, completions_text, solutions=None, rewards=None, reward_components=None):\n",
        "        if str(os.environ.get(\"RANK\", \"0\")) not in (\"0\", \"None\", \"\"):\n",
        "            return\n",
        "\n",
        "        with self.lock:\n",
        "            gs = -1\n",
        "            if self.trainer is not None and getattr(self.trainer, \"state\", None) is not None:\n",
        "                gs = int(getattr(self.trainer.state, \"global_step\", -1))\n",
        "\n",
        "            ts = time.time()\n",
        "            n = min(len(prompts), len(completions_text))\n",
        "\n",
        "            for i in range(n):\n",
        "                p = prompts[i]\n",
        "                user_text = None\n",
        "                if isinstance(p, list):\n",
        "                    for j in range(len(p) - 1, -1, -1):\n",
        "                        if isinstance(p[j], dict) and p[j].get(\"role\") == \"user\":\n",
        "                            user_text = p[j].get(\"content\")\n",
        "                            break\n",
        "\n",
        "                rec = {\n",
        "                    \"ts\": ts,\n",
        "                    \"global_step\": gs,\n",
        "                    \"task\": task,\n",
        "                    \"prompt_user\": user_text,\n",
        "                    \"completion\": completions_text[i],\n",
        "                    \"solution\": solutions[i] if isinstance(solutions, list) and i < len(solutions) else None,\n",
        "                    \"reward\": rewards[i] if isinstance(rewards, list) and i < len(rewards) else None,\n",
        "                    \"reward_components\": {\n",
        "                        k: (v[i] if isinstance(v, list) and i < len(v) else None)\n",
        "                        for k, v in (reward_components or {}).items()\n",
        "                    },\n",
        "                }\n",
        "                self._buf.append(json.dumps(rec, ensure_ascii=False))\n",
        "                self._lines += 1\n",
        "\n",
        "            if self._lines >= self.flush_every:\n",
        "                self.flush()\n",
        "\n",
        "    def flush(self):\n",
        "        if not self._buf:\n",
        "            return\n",
        "        self._fh.write(\"\\n\".join(self._buf) + \"\\n\")\n",
        "        self._fh.flush()\n",
        "        self._buf = []\n",
        "        self._lines = 0\n",
        "\n",
        "    def close(self):\n",
        "        try:\n",
        "            self.flush()\n",
        "        finally:\n",
        "            try:\n",
        "                self._fh.close()\n",
        "            except Exception:\n",
        "                pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49c7632b",
      "metadata": {
        "id": "49c7632b"
      },
      "source": [
        "## 7. Обучение GRPO\n",
        "\n",
        "Обёртка `run_task()`:\n",
        "- создаёт папку run’а,\n",
        "- настраивает `GRPOConfig`,\n",
        "- пишет reward-метрики в TensorBoard,\n",
        "- пишет генерации в `jsonl.gz` для последующего анализа.\n",
        "\n",
        "Доступно обучение на curriculum датасетах `interleave_datasets`, для этого нужно отключить `shuffle()` внутри `run_task`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2be3267",
      "metadata": {
        "id": "e2be3267"
      },
      "outputs": [],
      "source": [
        "import gc, torch\n",
        "import os, sys, logging\n",
        "from contextlib import redirect_stdout, redirect_stderr\n",
        "from pathlib import Path\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "\n",
        "def _configure_file_logging(run_dir: str):\n",
        "    \"\"\"Send Python logging (and most library logs) to files, not notebook output.\"\"\"\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    log_path = os.path.join(run_dir, \"console.log\")\n",
        "    err_path = os.path.join(run_dir, \"console.err\")\n",
        "\n",
        "    # Root logger -> file only\n",
        "    root = logging.getLogger()\n",
        "    root.setLevel(logging.INFO)\n",
        "\n",
        "    # Remove stream handlers (Jupyter attaches one by default)\n",
        "    for h in list(root.handlers):\n",
        "        if isinstance(h, logging.StreamHandler):\n",
        "            root.removeHandler(h)\n",
        "\n",
        "    fh = logging.FileHandler(log_path, mode=\"a\", encoding=\"utf-8\")\n",
        "    fh.setLevel(logging.INFO)\n",
        "    fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\")\n",
        "    fh.setFormatter(fmt)\n",
        "    root.addHandler(fh)\n",
        "\n",
        "    # Quiet noisy libraries\n",
        "    for name in [\"transformers\", \"trl\", \"datasets\", \"accelerate\", \"vllm\", \"pyngrok\", \"sqlglot\"]:\n",
        "        logging.getLogger(name).setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "    # Transformers/Datasets also have their own verbosity knobs\n",
        "    try:\n",
        "        from transformers.utils import logging as hf_logging\n",
        "        hf_logging.set_verbosity_error()\n",
        "        hf_logging.disable_progress_bar()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        from datasets.utils.logging import set_verbosity_error as ds_set_verbosity_error\n",
        "        ds_set_verbosity_error()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "    return log_path, err_path\n",
        "\n",
        "\n",
        "def cleanup_cuda():\n",
        "    gc.collect()\n",
        "    try:\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def build_args(run_dir, **overrides):\n",
        "    args = GRPOConfig(\n",
        "        use_vllm=True,\n",
        "        dataloader_drop_last=True,\n",
        "        learning_rate=1e-5,\n",
        "        adam_beta1=0.9,\n",
        "        adam_beta2=0.99,\n",
        "        weight_decay=0.1,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        optim=\"adamw_8bit\",\n",
        "        logging_steps=1,\n",
        "        disable_tqdm=True,\n",
        "        log_level=\"error\",\n",
        "        log_on_each_node=False,\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        per_device_train_batch_size=32,\n",
        "        gradient_accumulation_steps=1,\n",
        "        num_generations=16,\n",
        "        max_prompt_length=1024,\n",
        "        max_completion_length=2048,\n",
        "        max_steps=300,\n",
        "        save_steps=50,\n",
        "        save_total_limit=3,\n",
        "        save_strategy=\"steps\",\n",
        "        max_grad_norm=1.0,\n",
        "        report_to=[\"tensorboard\"],\n",
        "        logging_dir=run_dir,\n",
        "        output_dir=run_dir,\n",
        "    )\n",
        "    # overrides\n",
        "    for k, v in overrides.items():\n",
        "        setattr(args, k, v)\n",
        "    return args\n",
        "\n",
        "\n",
        "class CurriculumGRPOTrainer(GRPOTrainer):\n",
        "    def _get_train_sampler(self):\n",
        "        if getattr(self.args, \"world_size\", 1) > 1:\n",
        "            return DistributedSampler(self.train_dataset, shuffle=False, seed=self.args.seed)\n",
        "        return SequentialSampler(self.train_dataset)\n",
        "\n",
        "\n",
        "def run_task(task_name, task_dataset, base_run_dir=RUNS_DIR, run_id=\"001\", args_overrides=None, shuffle_dataset=True):\n",
        "    \"\"\"\n",
        "    task_name: 'sql' | 'math' | 'sudoku' | 'code' | 'ioi' | 'mixed'\n",
        "    task_dataset: sql_ds / math_ds / sudoku_ds (у тебя они уже есть)\n",
        "    \"\"\"\n",
        "    global REWARD_CACHE\n",
        "    reward_fn = make_reward_logged(task_name)\n",
        "    if args_overrides is None:\n",
        "        args_overrides = {}\n",
        "\n",
        "    RUN_DIR = f\"{base_run_dir}/grpo_{task_name}_run_{run_id}\"\n",
        "    Path(RUN_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    writer = SummaryWriter(log_dir=RUN_DIR)\n",
        "\n",
        "    # кэш rewards для логирования\n",
        "    REWARD_CACHE = RewardCache(task_name=task_name)\n",
        "\n",
        "    if task_name == \"math\":\n",
        "        args_overrides = {**{\"max_completion_length\": 2048}, **args_overrides}\n",
        "    if task_name == \"sql\":\n",
        "        args_overrides = {**{\"max_completion_length\": 2048}, **args_overrides}\n",
        "\n",
        "    if task_name == \"code\":\n",
        "        args_overrides = {**{\"max_completion_length\": 768}, **args_overrides}\n",
        "    if task_name == \"mixed\":\n",
        "        args_overrides = {**{\"max_prompt_length\": 1024, \"max_completion_length\": 2048}, **args_overrides}\n",
        "\n",
        "    training_args = build_args(RUN_DIR, **args_overrides)\n",
        "    global GEN_LOGGER\n",
        "    DRIVE_LOG_DIR = \"/content/drive/MyDrive/grpo_logs\"\n",
        "    RUN_ID = os.path.basename(RUN_DIR)\n",
        "    log_path = os.path.join(DRIVE_LOG_DIR, f\"{RUN_ID}_generations.jsonl.gz\")\n",
        "    GEN_LOGGER = SingleFileGenerationLogger(log_path, compress=True, flush_every_lines=200)\n",
        "\n",
        "\n",
        "    ds = task_dataset.shuffle(seed=SEED) if shuffle_dataset else task_dataset\n",
        "\n",
        "    cb = RewardTBCallback(writer=writer, task_name=task_name)\n",
        "\n",
        "    trainer = CurriculumGRPOTrainer(\n",
        "        model=model,\n",
        "        processing_class=tokenizer,\n",
        "        reward_funcs=[reward_fn],\n",
        "        args=training_args,\n",
        "        train_dataset=ds,\n",
        "        callbacks=[cb],\n",
        "    )\n",
        "\n",
        "    if GEN_LOGGER is not None:\n",
        "      GEN_LOGGER.attach_trainer(trainer)\n",
        "\n",
        "\n",
        "    # не печатаем в ноутбук: всё уходит в файлы в RUN_DIR\n",
        "    log_path, err_path = _configure_file_logging(RUN_DIR)\n",
        "    with open(log_path, \"a\", encoding=\"utf-8\") as f_out, open(err_path, \"a\", encoding=\"utf-8\") as f_err:\n",
        "        with redirect_stdout(f_out), redirect_stderr(f_err):\n",
        "            logging.info(f\"Starting GRPO for task={task_name} | run_dir={RUN_DIR} | n={len(ds)}\")\n",
        "            trainer.train()\n",
        "\n",
        "    writer.flush()\n",
        "    writer.close()\n",
        "\n",
        "    # можно сохранить финал, если нужно\n",
        "    # trainer.save_model(RUN_DIR)\n",
        "\n",
        "    cleanup_cuda()\n",
        "    return RUN_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab2987e3",
      "metadata": {
        "id": "ab2987e3"
      },
      "source": [
        "### 8.1 Запуск обучения (пример)\n",
        "  \n",
        "Подстройте `max_steps`, `max_prompt_length`, `max_completion_length` и лимиты датасетов под доступную VRAM.\n",
        "\n",
        "- `max_steps`: число шагов GRPO;\n",
        "- `max_prompt_length`: максимум токенов в prompt (от EDA);\n",
        "- `max_completion_length`: максимум токенов генерации (от задачи);\n",
        "\n",
        "Перед запуском обучения поднимаем TensorBoard.\n",
        "Во время GRPO важно мониторить:\n",
        "- средний reward и его компоненты,\n",
        "- `completion_length`, `kl`, `grad_norm`,\n",
        "- отдельные метрики по задачам (sql_exec, math_correct, code_pass).\n",
        "\n",
        "В Colab TensorBoard обычно недоступен извне, поэтому удобно использовать **ngrok-туннель**.\n",
        "\n",
        "Логика ниже без “убийства” процессов:\n",
        "\n",
        "- если TensorBoard уже запущен на порту — используем его;\n",
        "- если ранее уже был создан туннель и URL сохранён в файл — показываем этот URL (подключение к существующему);\n",
        "- иначе создаём новый туннель, если задан `NGROK_AUTHTOKEN`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorBoard (без inline-рендера в ноутбуке): запускаем сервер в фоне и отдаём через ngrok.\n",
        "import os, time, subprocess, signal, re, logging\n",
        "from pyngrok import ngrok\n",
        "\n",
        "TB_LOGDIR = \"/content/drive/MyDrive/HSE GRPO/runs\"\n",
        "TB_PORT = 6006\n",
        "TB_LOGFILE = os.path.join(TB_LOGDIR, \"_tensorboard_server.log\")\n",
        "TB_URL_FILE = os.path.join(TB_LOGDIR, \"_tensorboard_public_url.txt\")\n",
        "\n",
        "# 1) аккуратно гасим старые tensorboard-процессы\n",
        "try:\n",
        "    subprocess.run([\"pkill\", \"-f\", f\"tensorboard.*--port {TB_PORT}\"], check=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 2) стартуем TensorBoard в фоне (stdout/stderr -> файл)\n",
        "with open(TB_LOGFILE, \"a\", encoding=\"utf-8\") as f:\n",
        "    subprocess.Popen(\n",
        "        [\"tensorboard\", \"--logdir\", TB_LOGDIR, \"--host\", \"0.0.0.0\", \"--port\", str(TB_PORT)],\n",
        "        stdout=f, stderr=subprocess.STDOUT\n",
        "    )\n",
        "\n",
        "time.sleep(2.0)\n",
        "\n",
        "# 3) поднимаем ngrok-туннель (без вывода UI в ноутбук)\n",
        "logging.getLogger(\"pyngrok\").setLevel(logging.ERROR)\n",
        "public_url = ngrok.connect(TB_PORT, \"http\").public_url\n",
        "\n",
        "# 4) сохраняем URL в файл, чтобы не засорять вывод\n",
        "with open(TB_URL_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(public_url)\n",
        "\n",
        "# Ссылка на веб-интерфейс\n",
        "public_url\n"
      ],
      "metadata": {
        "id": "rY3Xf1E6AuVq"
      },
      "id": "rY3Xf1E6AuVq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcbe4ed7",
      "metadata": {
        "id": "fcbe4ed7"
      },
      "outputs": [],
      "source": [
        "mixed_run_dir = run_task(\n",
        "     task_name=\"mixed\",\n",
        "     task_dataset=train_dataset,\n",
        "     run_id=f\"mixed_{RUN_STAMP}\",\n",
        "     args_overrides={\n",
        "         \"max_steps\": 300,\n",
        "         \"per_device_train_batch_size\": 16,\n",
        "         \"num_generations\": 8,\n",
        "         \"max_prompt_length\": 1024,\n",
        "         \"max_completion_length\": 1024,\n",
        "     },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92379600",
      "metadata": {
        "id": "92379600"
      },
      "source": [
        "## 9. Сохранение модели и артефактов\n",
        "\n",
        "Что сохраняем по итогу:\n",
        "- LoRA адаптер (`save_pretrained`) — компактный артефакт обучения;\n",
        "- (опционально) merged модель;\n",
        "- jsonl.gz генераций и метрик (для анализа ошибок);\n",
        "- TensorBoard-логи (tfevents...).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5adbc6e",
      "metadata": {
        "id": "a5adbc6e"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()\n",
        "\n",
        "create_repo(\"username/repo\")\n",
        "\n",
        "model.save_pretrained(\"path\")\n",
        "tokenizer.save_pretrained(\"path\")\n",
        "\n",
        "model.push_to_hub(\"username/repo\")\n",
        "tokenizer.push_to_hub(\"username/repo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40f60ba2",
      "metadata": {
        "id": "40f60ba2"
      },
      "source": [
        "## 10. Итоги\n",
        "\n",
        "- Построен единый пайплайн GRPO для нескольких verifiable задач.\n",
        "- Реализованы reward-функции, поддерживающие автоматическую проверку корректности.\n",
        "- Добавлены инструменты логирования (TensorBoard + jsonl.gz) для анализа ошибок и прогресса.\n",
        "- Организованы детерминированные сплиты (Math) и фильтрация утечек (SQL).\n",
        "\n",
        "Возможности:\n",
        "- калибровка весов reward-компонент,\n",
        "- абляции (каждый reward по отдельности),\n",
        "- сравнение: отдельное обучение vs merge адаптеров,\n",
        "- финальный прогон бенчмарков (MBPP/HumanEval/Spider).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}